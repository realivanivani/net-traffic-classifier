{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook: HTTP Request Classifier\n",
    "\n",
    "This notebook demonstrates training a classifier to distinguish between malicious and normal HTTP requests using a labeled dataset.\n",
    "\n",
    "**Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Feel free to import other libraries as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was added, after suppling Gemini with a small sample of the training dataset file. The Data loading part was then adjusted (after several adjustments to the parsing and loading data functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data has space delimiter\n",
    "delimiter = \" \"\n",
    "\n",
    "def parse_http_request(lines):\n",
    "  \"\"\"\n",
    "  Parses a single HTTP request line into a dictionary.\n",
    "\n",
    "  Args:\n",
    "      lines (list): A list of strings representing the HTTP request lines (including headers).\n",
    "\n",
    "  Returns:\n",
    "      dict: A dictionary containing request method, URL, protocol, and headers (optional).\n",
    "  \"\"\"\n",
    "  parts = lines[0].split(\" \")\n",
    "  request = {\"method\": parts[0], \"url\": parts[1], \"protocol\": parts[2]}\n",
    "  headers = {}\n",
    "  for line in lines[1:]:\n",
    "      if line.strip():  # Check if line has content\n",
    "          key, value = line.split(\":\", 1)\n",
    "          headers[key.strip()] = value.strip()\n",
    "      else:\n",
    "          break  # Reached end of headers (or empty line)\n",
    "  request[\"headers\"] = headers if headers else None  # Set headers to None if empty\n",
    "  return request\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "  \"\"\"\n",
    "  Loads HTTP request data from a file.\n",
    "\n",
    "  Args:\n",
    "      filename (str): The filename of the data file.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of dictionaries, each representing an HTTP request.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  with open(filename, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "      request_data = parse_http_request(lines.copy())  # Pass a copy of lines\n",
    "      data.append(request_data)\n",
    "  return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data using the custom function\n",
    "normal_train_data = load_data(\"csic_dataset/normalTrafficTraining.txt\")\n",
    "normal_test_data = load_data(\"csic_dataset/normalTrafficTest.txt\")\n",
    "anomalous_test_data = load_data(\"csic_dataset/anomalousTrafficTest.txt\")\n",
    "\n",
    "# Combine normal and anomalous data for supervised learning\n",
    "all_data = normal_train_data + normal_test_data + anomalous_test_data\n",
    "all_data = pd.DataFrame(all_data)  # Convert to pandas dataframe for further processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This step is crucial for classifier performance and needs to be tailored to your data. Nonetheless, Gemini did not suggest any code for this part. So, I asked it to give me some solutions for feature engineering. Here is the output of that query:\n",
    "\n",
    "Here are some potential feature engineering solutions you can explore based on the parsed HTTP request data (dictionary format):\n",
    "\n",
    "**Basic Features:**\n",
    "\n",
    "* **Method:** Categorical feature indicating the HTTP method used (e.g., GET, POST, PUT, DELETE). You can encode this using one-hot encoding or label encoding.\n",
    "* **URL Path:** Extract the path component from the URL. This can reveal the targeted resource on the server. You can further process this by:\n",
    "    * Splitting the path into segments (e.g., \"/products/123\") and using them as features.\n",
    "    * Identifying specific patterns or keywords in the path that might be indicative of certain functionalities.\n",
    "* **Protocol Version:** Categorical feature indicating the HTTP protocol version (e.g., HTTP/1.1, HTTP/2).\n",
    "* **Status Code (if available):** This might be present in the response data, but if included in your request logs, it can be a valuable indicator of success or error conditions.\n",
    "* **Content Length (if available):** This might be present in the request headers, indicating the size of the request body (if any).\n",
    "\n",
    "**Header-based Features:**\n",
    "\n",
    "* **User-Agent:** Categorical feature indicating the browser or client making the request. You can potentially group similar user agents or identify known malicious ones.\n",
    "* **Referer:** This header indicates the previous webpage that linked to the current request. It can be helpful in understanding the context of the request.\n",
    "* **Content-Type (if available):** This indicates the format of the request body data (e.g., application/json, application/x-www-form-urlencoded).\n",
    "* **Number of Headers:** This can be a simple feature indicating the complexity of the request.\n",
    "\n",
    "**Advanced Features:**\n",
    "\n",
    "* **Request Frequency:** Analyze access logs to calculate the frequency of specific request patterns over time. Deviations from normal patterns might indicate suspicious activity.\n",
    "* **Time-based Features:** Extract features like hour of the day, day of the week, or even time zone from the timestamps (if available) in the logs. This can help identify unusual access patterns.\n",
    "* **IP Address Features:** Analyze source IP addresses and identify geographically anomalous origins for requests. Geolocation databases can be used for this purpose.\n",
    "\n",
    "**Remember:**\n",
    "\n",
    "* The choice of features depends on the specific characteristics of your data and the type of anomalies you are trying to detect.\n",
    "* Feature selection techniques like correlation analysis can help identify the most informative features for your model.\n",
    "* Consider data anonymization for sensitive information like IP addresses before processing, especially if sharing data for model development or evaluation.\n",
    "\n",
    "By exploring these feature engineering solutions and tailoring them to your data, you can create a more robust and informative feature set for training your HTTP request classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_data.drop(\"label\", axis=1)  # Features\n",
    "y = all_data[\"label\"]  # Labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier Training (Isolation Forest)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest is a good choice for anomaly detection\n",
    "clf = IsolationForest(contamination=0.1)  # Adjust contamination parameter as needed\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "# Additional evaluation metrics like precision, recall, F1 score can be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Further Exploration**\n",
    "\n",
    "* Try different anomaly detection algorithms (e.g., Local Outlier Factor, One-Class SVM) and compare their performance.\n",
    "* Experiment with feature selection techniques to identify the most informative features.\n",
    "* Visualize the distribution of features for normal and anomalous data to understand patterns.\n",
    "\n",
    "**Converting to Production**\n",
    "\n",
    "* Save the trained model using libraries like pickle or joblib.\n",
    "* Integrate the model into a web application or security system for real-time traffic classification.\n",
    "* Continuously monitor model performance and retrain with new data to maintain accuracy.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "* This is a basic example, and the specific implementation will depend on your data format and chosen features.\n",
    "* Remember to adapt the feature engineering and classifier selection based on your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
