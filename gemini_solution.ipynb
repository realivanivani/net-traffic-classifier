{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook: HTTP Request Classifier\n",
    "\n",
    "### Note about the prompt\n",
    "\n",
    "First, I just copied the text from the coding task. Later I added the names of the files and a sample of the dataset. As some parts of the code did not work properly (like parsing_http_request and load_data functions) after a few iterations and debugging, we got to the point where we made it work. Nonetheless, the Gemini first solution was a rough scatch in a good direction, but it had some major drawback that was making the code ineffective, which I commented in the notebook. Those should be further fixed using more prompts.\n",
    "\n",
    "Below is the prompt result:\n",
    "\n",
    "This notebook demonstrates training a classifier to distinguish between malicious and normal HTTP requests using a labeled dataset.\n",
    "\n",
    "**Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Feel free to import other libraries as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was added, after suppling Gemini with a small sample of the training dataset file. The Data loading part was then adjusted (after several adjustments to the parsing and loading data functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data has space delimiter\n",
    "delimiter = \" \"\n",
    "\n",
    "def parse_http_request(lines):\n",
    "  \"\"\"\n",
    "  Parses a single HTTP request line into a dictionary.\n",
    "\n",
    "  Args:\n",
    "      lines (list): A list of strings representing the HTTP request lines (including headers).\n",
    "\n",
    "  Returns:\n",
    "      dict: A dictionary containing request method, URL, protocol, and headers (optional).\n",
    "  \"\"\"\n",
    "  try:\n",
    "    parts = lines[0].split(\" \")\n",
    "    request = {\"method\": parts[0], \"url\": parts[1], \"protocol\": parts[2]}\n",
    "  except IndexError:\n",
    "    return {}\n",
    "  headers = {}\n",
    "  for line in lines[1:]:\n",
    "      if line.strip():  # Check if line has content\n",
    "          key, value = line.split(\":\", 1)\n",
    "          headers[key.strip()] = value.strip()\n",
    "      else:\n",
    "          break  # Reached end of headers (or empty line)\n",
    "  request[\"headers\"] = headers if headers else None  # Set headers to None if empty\n",
    "  return request\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "  \"\"\"\n",
    "  Loads HTTP request data from a file.\n",
    "\n",
    "  Args:\n",
    "      filename (str): The filename of the data file.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of dictionaries, each representing an HTTP request.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  current_request = []  # Create an empty list to store lines for a single request\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      # Check if line is empty (ignoring empty lines between requests)\n",
    "      if not line.strip():\n",
    "        if current_request:  # If current_request has lines, process it\n",
    "          data.append(parse_http_request(current_request))\n",
    "          current_request = []  # Reset for the next request\n",
    "      else:\n",
    "        current_request.append(line.strip())  # Add non-empty lines to current request\n",
    "  # Process the last request if it exists\n",
    "  if current_request:\n",
    "    data.append(parse_http_request(current_request))\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data using the custom function\n",
    "normal_train_data = load_data(\"csic_dataset/normalTrafficTraining.txt\")\n",
    "normal_test_data = load_data(\"csic_dataset/normalTrafficTest.txt\")\n",
    "anomalous_test_data = load_data(\"csic_dataset/anomalousTrafficTest.txt\")\n",
    "\n",
    "# Combine normal and anomalous data for supervised learning\n",
    "all_data = normal_train_data + normal_test_data + anomalous_test_data\n",
    "all_data = pd.DataFrame(all_data)  # Convert to pandas dataframe for further processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Gemini did not label the data, ie. it did not create a target column where normal data would be 0 and anomalous data would be 1. We definitely need to add this to make any sense of the training at all.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>protocol</th>\n",
       "      <th>headers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/index.jsp</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>{'User-Agent': 'Mozilla/5.0 (compatible; Konqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.j...</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>{'User-Agent': 'Mozilla/5.0 (compatible; Konqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.jsp</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>{'User-Agent': 'Mozilla/5.0 (compatible; Konqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/autentic...</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>{'User-Agent': 'Mozilla/5.0 (compatible; Konqu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method                                                url  protocol  \\\n",
       "0    GET            http://localhost:8080/tienda1/index.jsp  HTTP/1.1   \n",
       "1    GET  http://localhost:8080/tienda1/publico/anadir.j...  HTTP/1.1   \n",
       "2   POST   http://localhost:8080/tienda1/publico/anadir.jsp  HTTP/1.1   \n",
       "3    NaN                                                NaN       NaN   \n",
       "4    GET  http://localhost:8080/tienda1/publico/autentic...  HTTP/1.1   \n",
       "\n",
       "                                             headers  \n",
       "0  {'User-Agent': 'Mozilla/5.0 (compatible; Konqu...  \n",
       "1  {'User-Agent': 'Mozilla/5.0 (compatible; Konqu...  \n",
       "2  {'User-Agent': 'Mozilla/5.0 (compatible; Konqu...  \n",
       "3                                                NaN  \n",
       "4  {'User-Agent': 'Mozilla/5.0 (compatible; Konqu...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Gemini did not add any data exploration lines of code, so\n",
    "# I'm adding this part just to see the form of the final dataframe\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: We see that there are some errors in processing the input dataset, as there are NaN rows (4th row above). We definitely need to remove those, but Gemini forgot to include that step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.8 (like Gecko)',\n",
       " 'Pragma': 'no-cache',\n",
       " 'Cache-control': 'no-cache',\n",
       " 'Accept': 'text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5',\n",
       " 'Accept-Encoding': 'x-gzip, x-deflate, gzip, deflate',\n",
       " 'Accept-Charset': 'utf-8, utf-8;q=0.5, *;q=0.5',\n",
       " 'Accept-Language': 'en',\n",
       " 'Host': 'localhost:8080',\n",
       " 'Cookie': 'JSESSIONID=1F767F17239C9B670A39E9B10C3825F4',\n",
       " 'Connection': 'close'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.headers.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.8 (like Gecko)',\n",
       " 'Pragma': 'no-cache',\n",
       " 'Cache-control': 'no-cache',\n",
       " 'Accept': 'text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5',\n",
       " 'Accept-Encoding': 'x-gzip, x-deflate, gzip, deflate',\n",
       " 'Accept-Charset': 'utf-8, utf-8;q=0.5, *;q=0.5',\n",
       " 'Accept-Language': 'en',\n",
       " 'Host': 'localhost:8080',\n",
       " 'Cookie': 'JSESSIONID=EE6A06EE8174FC2B05432AA5A0FCE998',\n",
       " 'Content-Type': 'application/x-www-form-urlencoded',\n",
       " 'Connection': 'close',\n",
       " 'Content-Length': '33'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.headers.loc[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the method above did not disect the headers part, as well the url column. Processing the two column is definitely needed for a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding missing code\n",
    "\n",
    "Here, I added manually, all the things that were missing, since the rest of the code would not run at all if the data was not processed correctly.\n",
    "\n",
    "First, I added the target column before concating the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>protocol</th>\n",
       "      <th>headers</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35037</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/miembros/editar.jsp</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>{'User-Agent': 'Mozilla/5.0 (compatible; Konqu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35038</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35039</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/miembros/editar....</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>{'User-Agent': 'Mozilla/5.0 (compatible; Konqu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35040</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/imagenes/3.gif/</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>{'User-Agent': 'Mozilla/5.0 (compatible; Konqu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35041</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/imagenes/nuestra...</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>{'User-Agent': 'Mozilla/5.0 (compatible; Konqu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      method                                                url  protocol  \\\n",
       "35037   POST  http://localhost:8080/tienda1/miembros/editar.jsp  HTTP/1.1   \n",
       "35038    NaN                                                NaN       NaN   \n",
       "35039    GET  http://localhost:8080/tienda1/miembros/editar....  HTTP/1.1   \n",
       "35040    GET      http://localhost:8080/tienda1/imagenes/3.gif/  HTTP/1.1   \n",
       "35041    GET  http://localhost:8080/tienda1/imagenes/nuestra...  HTTP/1.1   \n",
       "\n",
       "                                                 headers  target  \n",
       "35037  {'User-Agent': 'Mozilla/5.0 (compatible; Konqu...       1  \n",
       "35038                                                NaN       1  \n",
       "35039  {'User-Agent': 'Mozilla/5.0 (compatible; Konqu...       1  \n",
       "35040  {'User-Agent': 'Mozilla/5.0 (compatible; Konqu...       1  \n",
       "35041  {'User-Agent': 'Mozilla/5.0 (compatible; Konqu...       1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To correct this we will first add the target column before concating the dataframes\n",
    "normal_train = pd.DataFrame(normal_train_data)\n",
    "normal_train['target'] = 0\n",
    "normal_test = pd.DataFrame(normal_test_data)\n",
    "normal_test['target'] = 0\n",
    "anomalous_test = pd.DataFrame(anomalous_test_data)\n",
    "anomalous_test['target'] = 1\n",
    "\n",
    "# Combine normal and anomalous data for supervised learning\n",
    "all_data = pd.concat([normal_train,normal_test,anomalous_test])\n",
    "all_data.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I removed NaN rows\n",
    "\n",
    "all_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This step is crucial for classifier performance and needs to be tailored to your data. Nonetheless, Gemini did not suggest any code for this part. So, I asked it to give me some solutions for feature engineering. Here is the output of that query:\n",
    "\n",
    "Here are some potential feature engineering solutions you can explore based on the parsed HTTP request data (dictionary format):\n",
    "\n",
    "**Basic Features:**\n",
    "\n",
    "* **Method:** Categorical feature indicating the HTTP method used (e.g., GET, POST, PUT, DELETE). You can encode this using one-hot encoding or label encoding.\n",
    "* **URL Path:** Extract the path component from the URL. This can reveal the targeted resource on the server. You can further process this by:\n",
    "    * Splitting the path into segments (e.g., \"/products/123\") and using them as features.\n",
    "    * Identifying specific patterns or keywords in the path that might be indicative of certain functionalities.\n",
    "* **Protocol Version:** Categorical feature indicating the HTTP protocol version (e.g., HTTP/1.1, HTTP/2).\n",
    "* **Status Code (if available):** This might be present in the response data, but if included in your request logs, it can be a valuable indicator of success or error conditions.\n",
    "* **Content Length (if available):** This might be present in the request headers, indicating the size of the request body (if any).\n",
    "\n",
    "**Header-based Features:**\n",
    "\n",
    "* **User-Agent:** Categorical feature indicating the browser or client making the request. You can potentially group similar user agents or identify known malicious ones.\n",
    "* **Referer:** This header indicates the previous webpage that linked to the current request. It can be helpful in understanding the context of the request.\n",
    "* **Content-Type (if available):** This indicates the format of the request body data (e.g., application/json, application/x-www-form-urlencoded).\n",
    "* **Number of Headers:** This can be a simple feature indicating the complexity of the request.\n",
    "\n",
    "**Advanced Features:**\n",
    "\n",
    "* **Request Frequency:** Analyze access logs to calculate the frequency of specific request patterns over time. Deviations from normal patterns might indicate suspicious activity.\n",
    "* **Time-based Features:** Extract features like hour of the day, day of the week, or even time zone from the timestamps (if available) in the logs. This can help identify unusual access patterns.\n",
    "* **IP Address Features:** Analyze source IP addresses and identify geographically anomalous origins for requests. Geolocation databases can be used for this purpose.\n",
    "\n",
    "**Remember:**\n",
    "\n",
    "* The choice of features depends on the specific characteristics of your data and the type of anomalies you are trying to detect.\n",
    "* Feature selection techniques like correlation analysis can help identify the most informative features for your model.\n",
    "* Consider data anonymization for sensitive information like IP addresses before processing, especially if sharing data for model development or evaluation.\n",
    "\n",
    "By exploring these feature engineering solutions and tailoring them to your data, you can create a more robust and informative feature set for training your HTTP request classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['label'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mall_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Features\u001b[39;00m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m all_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Labels\u001b[39;00m\n\u001b[1;32m      4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4827\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4825\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4827\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4828\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4830\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['label'] not found in axis\""
     ]
    }
   ],
   "source": [
    "X = all_data.drop(\"label\", axis=1)  # Features\n",
    "y = all_data[\"label\"]  # Labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier Training (Isolation Forest)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest is a good choice for anomaly detection\n",
    "clf = IsolationForest(contamination=0.1)  # Adjust contamination parameter as needed\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "# Additional evaluation metrics like precision, recall, F1 score can be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Further Exploration**\n",
    "\n",
    "* Try different anomaly detection algorithms (e.g., Local Outlier Factor, One-Class SVM) and compare their performance.\n",
    "* Experiment with feature selection techniques to identify the most informative features.\n",
    "* Visualize the distribution of features for normal and anomalous data to understand patterns.\n",
    "\n",
    "**Converting to Production**\n",
    "\n",
    "* Save the trained model using libraries like pickle or joblib.\n",
    "* Integrate the model into a web application or security system for real-time traffic classification.\n",
    "* Continuously monitor model performance and retrain with new data to maintain accuracy.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "* This is a basic example, and the specific implementation will depend on your data format and chosen features.\n",
    "* Remember to adapt the feature engineering and classifier selection based on your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
