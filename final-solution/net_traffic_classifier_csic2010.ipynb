{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection - HTTP requests - CSIC dataset 2010\n",
    "\n",
    "## Task\n",
    "The primary objective is to develop a classifier capable of identifying malicious HTTP requests by training on normal traffic data and evaluating both normal and anomalous test data. The dataset is structured as follows:\n",
    "\n",
    " * Normal Traffic (Train)\n",
    " * Normal Traffic (Test)\n",
    " * Anomalous Traffic (Test)\n",
    "\n",
    "Although the dataset is designed for unsupervised learning, supervised learning techniques can be applied by combining normal and anomalous data into a labeled dataset. This allows for direct classification using any preferred machine learning model.\n",
    "\n",
    "## Dataset\n",
    "The dataset contains the generated traffic targeted to an e-commerce web\n",
    "application. It is an automatically generated dataset that contains 36,000 normal\n",
    "requests and more than 25,000 anomalous requests (i.e., web attacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 00:12:56.091222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Feature importance and explainability\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# URL processing\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Scikit-learn preprocessing and model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_score, accuracy_score, recall_score, f1_score, \n",
    "    roc_auc_score, mean_absolute_error, confusion_matrix, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model selection and hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Deep Neural Networks (DNN) with TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Explainability tools\n",
    "from sklearn.inspection import PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csic_dataset/normalTrafficTraining.txt.gz\n",
      "csic_dataset/anomalousTrafficTest.txt\n",
      "csic_dataset/.DS_Store\n",
      "csic_dataset/normalTrafficTest.txt.gz\n",
      "csic_dataset/normalTrafficTraining.txt\n",
      "csic_dataset/anomalousTrafficTest.txt.gz\n",
      "csic_dataset/normalTrafficTest.txt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List all files under the specified directory (in this case, '/csic_dataset')\n",
    "for dirname, _, filenames in os.walk('csic_dataset/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Confirmation that the operation has completed\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data - Data prep\n",
    "\n",
    "Since the files are in .txt format, we would need to process them in order to get something useful like a table or a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to parse HTTP requests from a text block\n",
    "def parse_http_request(text):\n",
    "    # Split by double newline to separate headers and body\n",
    "    parts = text.strip().split('\\n\\n')\n",
    "    \n",
    "    # Initialize dictionary for storing parsed data\n",
    "    request_data = {}\n",
    "\n",
    "    # Extract the request line (first line)\n",
    "    request_line = parts[0].splitlines()[0]\n",
    "    method, url, http_version = request_line.split(' ', 2)\n",
    "    request_data['Method'] = method\n",
    "    request_data['URL'] = url\n",
    "    request_data['HTTP_Version'] = http_version\n",
    "\n",
    "    # Extract headers (remaining lines in the first part)\n",
    "    headers = parts[0].splitlines()[1:]\n",
    "    for header in headers:\n",
    "        key, value = header.split(': ', 1)\n",
    "        request_data[key] = value\n",
    "\n",
    "    # Extract the body if it exists\n",
    "    if len(parts) > 1:\n",
    "        request_data['Body'] = parts[1]\n",
    "    else:\n",
    "        request_data['Body'] = ''\n",
    "\n",
    "    return request_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved\n"
     ]
    }
   ],
   "source": [
    "# Folder containing the .txt files\n",
    "folder_path = 'csic_dataset/'\n",
    "\n",
    "# List to store all parsed request data\n",
    "all_requests = []\n",
    "\n",
    "# Iterate through all .txt files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        # Determine Train_Test and Normal_Anom from the filename\n",
    "        if 'Training' in filename:\n",
    "            train_test = 'Training'\n",
    "        elif 'Test' in filename:\n",
    "            train_test = 'Test'\n",
    "        else:\n",
    "            train_test = 'Unknown'  # In case the filename doesn't match\n",
    "\n",
    "        if 'normal' in filename:\n",
    "            normal_anom = 'Normal'\n",
    "        elif 'anomalous' in filename:\n",
    "            normal_anom = 'Anomalous'\n",
    "        else:\n",
    "            normal_anom = 'Unknown'  # In case the filename doesn't match\n",
    "\n",
    "        with open(os.path.join(folder_path, filename), 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "            # Split the text into individual HTTP requests (based on 'GET' or 'POST')\n",
    "            requests = re.split(r'\\n(?=GET|POST)', text)\n",
    "            for request in requests:\n",
    "                parsed_data = parse_http_request(request)\n",
    "                parsed_data['Train_Test'] = train_test\n",
    "                parsed_data['Normal_Anom'] = normal_anom\n",
    "                all_requests.append(parsed_data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(all_requests)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('http_requests_all.csv', index=False)\n",
    "\n",
    "\n",
    "# Now, since the assignment specified that the model should be trained only on the normalTrafficTraining.txt file, we'll split it by train and test\n",
    "df_train = df[df.Train_Test == 'Training']\n",
    "df_train.to_csv('http_requests_train.csv', index=False)\n",
    "\n",
    "df_test = df[(df.Train_Test == 'Test')]\n",
    "df_test.to_csv('http_requests_test.csv', index=False)\n",
    "#df[(df.Train_Test == 'Test') & (df.Normal_Anom == 'Anomalous')].to_csv('http_requests_test_anom.csv', index=False)\n",
    "\n",
    "print(f\"Data has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>URL</th>\n",
       "      <th>HTTP_Version</th>\n",
       "      <th>User-Agent</th>\n",
       "      <th>Pragma</th>\n",
       "      <th>Cache-control</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Accept-Encoding</th>\n",
       "      <th>Accept-Charset</th>\n",
       "      <th>Accept-Language</th>\n",
       "      <th>Host</th>\n",
       "      <th>Cookie</th>\n",
       "      <th>Connection</th>\n",
       "      <th>Body</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Normal_Anom</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Content-Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.j...</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=B92A8B48B9008CD29F622A994E0F650D</td>\n",
       "      <td>close</td>\n",
       "      <td></td>\n",
       "      <td>Test</td>\n",
       "      <td>Anomalous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.jsp</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=AE29AEEBDE479D5E1A18B4108C8E3CE0</td>\n",
       "      <td>close</td>\n",
       "      <td>id=2&amp;nombre=Jam%F3n+Ib%E9rico&amp;precio=85&amp;cantid...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Anomalous</td>\n",
       "      <td>application/x-www-form-urlencoded</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.j...</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=F563B5262843F12ECAE41815ABDEEA54</td>\n",
       "      <td>close</td>\n",
       "      <td></td>\n",
       "      <td>Test</td>\n",
       "      <td>Anomalous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.jsp</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=3B654D6DF7F1466EE80D7F756B00E5D1</td>\n",
       "      <td>close</td>\n",
       "      <td>id=2%2F&amp;nombre=Jam%F3n+Ib%E9rico&amp;precio=85&amp;can...</td>\n",
       "      <td>Test</td>\n",
       "      <td>Anomalous</td>\n",
       "      <td>application/x-www-form-urlencoded</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/asf-logo-wide.gif~</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=51A7470173188BBB993947F2283059E4</td>\n",
       "      <td>close</td>\n",
       "      <td></td>\n",
       "      <td>Test</td>\n",
       "      <td>Anomalous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Method                                                URL HTTP_Version  \\\n",
       "0    GET  http://localhost:8080/tienda1/publico/anadir.j...     HTTP/1.1   \n",
       "1   POST   http://localhost:8080/tienda1/publico/anadir.jsp     HTTP/1.1   \n",
       "2    GET  http://localhost:8080/tienda1/publico/anadir.j...     HTTP/1.1   \n",
       "3   POST   http://localhost:8080/tienda1/publico/anadir.jsp     HTTP/1.1   \n",
       "4    GET           http://localhost:8080/asf-logo-wide.gif~     HTTP/1.1   \n",
       "\n",
       "                                          User-Agent    Pragma Cache-control  \\\n",
       "0  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache      no-cache   \n",
       "1  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache      no-cache   \n",
       "2  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache      no-cache   \n",
       "3  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache      no-cache   \n",
       "4  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache      no-cache   \n",
       "\n",
       "                                              Accept  \\\n",
       "0  text/xml,application/xml,application/xhtml+xml...   \n",
       "1  text/xml,application/xml,application/xhtml+xml...   \n",
       "2  text/xml,application/xml,application/xhtml+xml...   \n",
       "3  text/xml,application/xml,application/xhtml+xml...   \n",
       "4  text/xml,application/xml,application/xhtml+xml...   \n",
       "\n",
       "                    Accept-Encoding               Accept-Charset  \\\n",
       "0  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "1  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "2  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "3  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "4  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "\n",
       "  Accept-Language            Host  \\\n",
       "0              en  localhost:8080   \n",
       "1              en  localhost:8080   \n",
       "2              en  localhost:8080   \n",
       "3              en  localhost:8080   \n",
       "4              en  localhost:8080   \n",
       "\n",
       "                                        Cookie Connection  \\\n",
       "0  JSESSIONID=B92A8B48B9008CD29F622A994E0F650D      close   \n",
       "1  JSESSIONID=AE29AEEBDE479D5E1A18B4108C8E3CE0      close   \n",
       "2  JSESSIONID=F563B5262843F12ECAE41815ABDEEA54      close   \n",
       "3  JSESSIONID=3B654D6DF7F1466EE80D7F756B00E5D1      close   \n",
       "4  JSESSIONID=51A7470173188BBB993947F2283059E4      close   \n",
       "\n",
       "                                                Body Train_Test Normal_Anom  \\\n",
       "0                                                          Test   Anomalous   \n",
       "1  id=2&nombre=Jam%F3n+Ib%E9rico&precio=85&cantid...       Test   Anomalous   \n",
       "2                                                          Test   Anomalous   \n",
       "3  id=2%2F&nombre=Jam%F3n+Ib%E9rico&precio=85&can...       Test   Anomalous   \n",
       "4                                                          Test   Anomalous   \n",
       "\n",
       "                        Content-Type Content-Length  \n",
       "0                                NaN            NaN  \n",
       "1  application/x-www-form-urlencoded            146  \n",
       "2                                NaN            NaN  \n",
       "3  application/x-www-form-urlencoded             77  \n",
       "4                                NaN            NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Anomalous', 'Normal'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Normal_Anom.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Test', 'Training'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Train_Test.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since the assignment specified that the model should be trained only on the normalTrafficTraining.txt file, we will first try only with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 96668\n",
      "Number of features: 18\n"
     ]
    }
   ],
   "source": [
    "n_features=df.shape[1]\n",
    "n_samples=df.shape[0]\n",
    "\n",
    "print(\"Number of samples:\", n_samples)\n",
    "print(\"Number of features:\", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>URL</th>\n",
       "      <th>HTTP_Version</th>\n",
       "      <th>User-Agent</th>\n",
       "      <th>Pragma</th>\n",
       "      <th>Cache-control</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Accept-Encoding</th>\n",
       "      <th>Accept-Charset</th>\n",
       "      <th>Accept-Language</th>\n",
       "      <th>Host</th>\n",
       "      <th>Cookie</th>\n",
       "      <th>Connection</th>\n",
       "      <th>Body</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Normal_Anom</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Content-Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96663</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/imagenes/2.gif</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=E1E16AC490F40B8484CD75E2DBE32075</td>\n",
       "      <td>close</td>\n",
       "      <td></td>\n",
       "      <td>Test</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96664</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/imagenes/3.gif</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=4567793E184E0925234DADCEECD6999A</td>\n",
       "      <td>close</td>\n",
       "      <td></td>\n",
       "      <td>Test</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96665</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/imagenes/cmenbul...</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=487FD70FECB4D14155C95F38C389DA0D</td>\n",
       "      <td>close</td>\n",
       "      <td></td>\n",
       "      <td>Test</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96666</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/imagenes/logo.gif</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=6E0F5F3BC982DFC73B39EAD495ADCE96</td>\n",
       "      <td>close</td>\n",
       "      <td></td>\n",
       "      <td>Test</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96667</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/imagenes/nuestra...</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>localhost:8080</td>\n",
       "      <td>JSESSIONID=A70DD1BA160B294CB5E1C2D8FAE7C09F</td>\n",
       "      <td>close</td>\n",
       "      <td></td>\n",
       "      <td>Test</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Method                                                URL HTTP_Version  \\\n",
       "96663    GET       http://localhost:8080/tienda1/imagenes/2.gif     HTTP/1.1   \n",
       "96664    GET       http://localhost:8080/tienda1/imagenes/3.gif     HTTP/1.1   \n",
       "96665    GET  http://localhost:8080/tienda1/imagenes/cmenbul...     HTTP/1.1   \n",
       "96666    GET    http://localhost:8080/tienda1/imagenes/logo.gif     HTTP/1.1   \n",
       "96667    GET  http://localhost:8080/tienda1/imagenes/nuestra...     HTTP/1.1   \n",
       "\n",
       "                                              User-Agent    Pragma  \\\n",
       "96663  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache   \n",
       "96664  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache   \n",
       "96665  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache   \n",
       "96666  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache   \n",
       "96667  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  no-cache   \n",
       "\n",
       "      Cache-control                                             Accept  \\\n",
       "96663      no-cache  text/xml,application/xml,application/xhtml+xml...   \n",
       "96664      no-cache  text/xml,application/xml,application/xhtml+xml...   \n",
       "96665      no-cache  text/xml,application/xml,application/xhtml+xml...   \n",
       "96666      no-cache  text/xml,application/xml,application/xhtml+xml...   \n",
       "96667      no-cache  text/xml,application/xml,application/xhtml+xml...   \n",
       "\n",
       "                        Accept-Encoding               Accept-Charset  \\\n",
       "96663  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "96664  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "96665  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "96666  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "96667  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "\n",
       "      Accept-Language            Host  \\\n",
       "96663              en  localhost:8080   \n",
       "96664              en  localhost:8080   \n",
       "96665              en  localhost:8080   \n",
       "96666              en  localhost:8080   \n",
       "96667              en  localhost:8080   \n",
       "\n",
       "                                            Cookie Connection Body Train_Test  \\\n",
       "96663  JSESSIONID=E1E16AC490F40B8484CD75E2DBE32075      close            Test   \n",
       "96664  JSESSIONID=4567793E184E0925234DADCEECD6999A      close            Test   \n",
       "96665  JSESSIONID=487FD70FECB4D14155C95F38C389DA0D      close            Test   \n",
       "96666  JSESSIONID=6E0F5F3BC982DFC73B39EAD495ADCE96      close            Test   \n",
       "96667  JSESSIONID=A70DD1BA160B294CB5E1C2D8FAE7C09F      close            Test   \n",
       "\n",
       "      Normal_Anom Content-Type Content-Length  \n",
       "96663      Normal          NaN            NaN  \n",
       "96664      Normal          NaN            NaN  \n",
       "96665      Normal          NaN            NaN  \n",
       "96666      Normal          NaN            NaN  \n",
       "96667      Normal          NaN            NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Method', 'URL', 'HTTP_Version', 'User-Agent', 'Pragma',\n",
       "       'Cache-control', 'Accept', 'Accept-Encoding', 'Accept-Charset',\n",
       "       'Accept-Language', 'Host', 'Cookie', 'Connection', 'Body', 'Train_Test',\n",
       "       'Normal_Anom', 'Content-Type', 'Content-Length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.countplot(data=csic_data, x='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping samples with at least 1 NaN value will make to lose all the other Request Methods besides POST, this option is discarded since dropping data is not usually a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing URL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=[ 'Unnamed: 0','Method', 'User-Agent', 'Pragma', 'Cache-Control',\n",
    "       'Accept', 'Accept-encoding', 'Accept-charset', 'language', 'host',\n",
    "       'cookie', 'content-type', 'connection', 'lenght', 'content','classification',\n",
    "        'URL']\n",
    "\n",
    "X=csic_data[feature_names]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Removing not discriminatory features**\n",
    "\n",
    "**Enumerating unique values for each feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing not discriminatory Features and making some adjustments on feature names\n",
    "X = X.rename(columns={'Unnamed: 0': 'Class'})\n",
    "X = X.rename(columns={'lenght': 'content_length'})\n",
    "\n",
    "\n",
    "feature_names=[ 'Class','Method','host','cookie','Accept', 'content_length', 'content','classification','URL']\n",
    "\n",
    "# Print the remaining data\n",
    "X = X[feature_names]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=X.Class\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=X.shape[1]\n",
    "# Get list of categorical variables\n",
    "s = (X.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-processing on the feature: Content Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.content_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations on the 'content_lenght' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaN values with 0\n",
    "#removing the 'Content-Lenght' string and keeping only the numerical value\n",
    "\n",
    "X['content_length'] = X['content_length'].astype(str)\n",
    "X['content_length'] = X['content_length'].str.extract(r'(\\d+)')\n",
    "X['content_length'] = pd.to_numeric(X['content_length'], errors='coerce').fillna(0)\n",
    "print(X.content_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET methods have the content_length set to 0 since they where all NaN (this method does not have to provide content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_length = X.loc[X['Method'] == 'GET', 'content_length']\n",
    "print(filtered_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_counts = X['URL'].value_counts()\n",
    "most_common_urls = url_counts.head(10)  # Extract the top 10 most common strings\n",
    "\n",
    "print(\"Most common URLs:\")\n",
    "for i, (url, count) in enumerate(most_common_urls.items(), 1):\n",
    "    print(f\"{i}. URL: {url} - Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils for URL/Content pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dot(url):\n",
    "    count_dot = url.count('.')\n",
    "    return count_dot\n",
    "\n",
    "\n",
    "def no_of_dir(url):\n",
    "    urldir = urlparse(url).path\n",
    "    return urldir.count('/')\n",
    "\n",
    "def no_of_embed(url):\n",
    "    urldir = urlparse(url).path\n",
    "    return urldir.count('//')\n",
    "\n",
    "def shortening_service(url):\n",
    "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
    "                      'tr\\.im|link\\.zip\\.net',\n",
    "                      url)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def count_http(url):\n",
    "    return url.count('http')\n",
    "\n",
    "def count_per(url):\n",
    "    return url.count('%')\n",
    "\n",
    "def count_ques(url):\n",
    "    return url.count('?')\n",
    "\n",
    "def count_hyphen(url):\n",
    "    return url.count('-')\n",
    "\n",
    "\n",
    "def count_equal(url):\n",
    "    return url.count('=')\n",
    "\n",
    "\n",
    "def url_length(url):\n",
    "    return len(str(url))\n",
    "\n",
    "#Hostname Length\n",
    "\n",
    "def hostname_length(url):\n",
    "    return len(urlparse(url).netloc)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def suspicious_words(url):\n",
    "    score_map = {\n",
    "        'error': 30,\n",
    "        'errorMsg': 30,\n",
    "        'id': 10,\n",
    "        'errorID': 30,\n",
    "        'SELECT': 50,\n",
    "        'FROM': 50,\n",
    "        'WHERE': 50,\n",
    "        'DELETE': 50,\n",
    "        'USERS': 50,\n",
    "        'DROP': 50,\n",
    "        'CREATE': 50,\n",
    "        'INJECTED': 50,\n",
    "        'TABLE': 50,\n",
    "        'alert': 30,\n",
    "        'javascript': 20,\n",
    "        'cookie': 25,\n",
    "        '--': 30,\n",
    "        '.exe': 30,\n",
    "        '.php': 20,\n",
    "        '.js': 10,\n",
    "        'admin': 10,\n",
    "        'administrator': 10,\n",
    "        '\\'': 30,\n",
    "        'password': 15,\n",
    "        'login': 15,\n",
    "        'incorrect': 20,\n",
    "        'pwd': 15,\n",
    "        'tamper': 25,\n",
    "        'vaciar': 20,\n",
    "        'carrito': 25,\n",
    "        'wait': 30,\n",
    "        'delay': 35,\n",
    "        'set': 20,\n",
    "        'steal': 35,\n",
    "        'hacker': 35,\n",
    "        'proxy': 35,\n",
    "        'location': 30,\n",
    "        'document.cookie': 40,\n",
    "        'document': 20,\n",
    "        'set-cookie': 40,\n",
    "        'create': 40,\n",
    "        'cmd': 40,\n",
    "        'dir': 30,\n",
    "        'shell': 40,\n",
    "        'reverse': 30,\n",
    "        'bin': 20,\n",
    "        'cookiesteal': 40,\n",
    "        'LIKE': 30,\n",
    "        'UNION': 35,\n",
    "        'include': 30,\n",
    "        'file': 20,\n",
    "        'tmp': 25,\n",
    "        'ssh': 40,\n",
    "        'exec': 30,\n",
    "        'cat': 25,\n",
    "        'etc': 30,\n",
    "        'fetch': 25,\n",
    "        'eval': 30,\n",
    "        'wait': 30,\n",
    "        'malware': 45,\n",
    "        'ransomware': 45,\n",
    "        'phishing': 45,\n",
    "        'exploit': 45,\n",
    "        'virus': 45,\n",
    "        'trojan': 45,\n",
    "        'backdoor': 45,\n",
    "        'spyware': 45,\n",
    "        'rootkit': 45,\n",
    "        'credential': 30,\n",
    "        'inject': 30,\n",
    "        'script': 25,\n",
    "        'iframe': 25,\n",
    "        'src=': 25,\n",
    "        'onerror': 30,\n",
    "        'prompt': 20,\n",
    "        'confirm': 20,\n",
    "        'eval': 25,\n",
    "        'expression': 30,\n",
    "        'function\\(': 20,\n",
    "        'xmlhttprequest': 30,\n",
    "        'xhr': 20,\n",
    "        'window.': 20,\n",
    "        'document.': 20,\n",
    "        'cookie': 25,\n",
    "        'click': 15,\n",
    "        'mouseover': 15,\n",
    "        'onload': 20,\n",
    "        'onunload': 20,\n",
    "    }\n",
    "\n",
    "    matches = re.findall(r'(?i)' + '|'.join(score_map.keys()), url)\n",
    "\n",
    "    total_score = sum(score_map.get(match.lower(), 0) for match in matches)\n",
    "    return total_score\n",
    "\n",
    "\n",
    "def digit_count(url):\n",
    "    digits = 0\n",
    "    for i in url:\n",
    "        if i.isnumeric():\n",
    "            digits = digits + 1\n",
    "    return digits\n",
    "\n",
    "def letter_count(url):\n",
    "    letters = 0\n",
    "    for i in url:\n",
    "        if i.isalpha():\n",
    "            letters += 1\n",
    "    return letters\n",
    "\n",
    "def count_special_characters(url):\n",
    "    special_characters = re.sub(r'[a-zA-Z0-9\\s]', '', url)\n",
    "    count = len(special_characters)\n",
    "    return count\n",
    "\n",
    "\n",
    "# Number of Parameters in URL\n",
    "def number_of_parameters(url):\n",
    "    params = urlparse(url).query\n",
    "    return 0 if params == '' else len(params.split('&'))\n",
    "\n",
    "# Number of Fragments in URL\n",
    "def number_of_fragments(url):\n",
    "    frags = urlparse(url).fragment\n",
    "    return len(frags.split('#')) - 1 if frags == '' else 0\n",
    "\n",
    "# URL is Encoded\n",
    "def is_encoded(url):\n",
    "    return int('%' in url.lower())\n",
    "\n",
    "\n",
    "def unusual_character_ratio(url):\n",
    "    total_characters = len(url)\n",
    "    unusual_characters = re.sub(r'[a-zA-Z0-9\\s\\-._]', '', url)\n",
    "    unusual_count = len(unusual_characters)\n",
    "    ratio = unusual_count / total_characters if total_characters > 0 else 0\n",
    "    return ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['count_dot_url'] = X['URL'].apply(count_dot)\n",
    "X['count_dir_url'] = X['URL'].apply(no_of_dir)\n",
    "X['count_embed_domain_url'] = X['URL'].apply(no_of_embed)\n",
    "X['short_url'] = X['URL'].apply(shortening_service)\n",
    "X['count-http'] = X['URL'].apply(count_http)\n",
    "X['count%_url'] = X['URL'].apply(count_per)\n",
    "X['count?_url'] = X['URL'].apply(count_ques)\n",
    "X['count-_url'] = X['URL'].apply(count_hyphen)\n",
    "X['count=_url'] = X['URL'].apply(count_equal)\n",
    "X['hostname_length_url'] = X['URL'].apply(hostname_length)\n",
    "X['sus_url'] = X['URL'].apply(suspicious_words)\n",
    "X['count-digits_url'] = X['URL'].apply(digit_count)\n",
    "X['count-letters_url'] = X['URL'].apply(letter_count)\n",
    "X['url_length'] = X['URL'].apply(url_length)\n",
    "X['number_of_parameters_url'] = X['URL'].apply(number_of_parameters)\n",
    "X['number_of_fragments_url'] = X['URL'].apply(number_of_fragments)\n",
    "X['is_encoded_url'] = X['URL'].apply(is_encoded)\n",
    "X['special_count_url'] = X['URL'].apply(count_special_characters)\n",
    "X['unusual_character_ratio_url'] = X['URL'].apply(unusual_character_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features and class variable for plotting\n",
    "new_features = ['count_dot_url', 'count_dir_url', 'count_embed_domain_url', 'count-http',\n",
    "                'count%_url', 'count?_url', 'count-_url', 'count=_url', 'url_length', 'hostname_length_url',\n",
    "                'sus_url', 'count-digits_url', 'count-letters_url', 'number_of_parameters_url',\n",
    "                'number_of_fragments_url', 'is_encoded_url','special_count_url','unusual_character_ratio_url']\n",
    "\n",
    "# Create a DataFrame with the selected features\n",
    "set = X[new_features]\n",
    "\n",
    "for new_feature in X.columns:\n",
    "    if new_feature in X.columns:\n",
    "        unique_count = X[new_feature].nunique()\n",
    "        print(f\"Number of unique values for {new_feature}: {unique_count}\")\n",
    "    else:\n",
    "        print(f\"Column '{new_feature}' does not exist in the DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Cookies as feature\n",
    " **cookies are unique for each sample, this feature cannot be used as discriminant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = X['cookie'].nunique()\n",
    "print(f\"Count of unique values in 'cookie': {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Accept'] = X['Accept'].astype(str)\n",
    "X['Accept'] = X['Accept'].str.extract(r'(\\d+)')\n",
    "X['Accept'] = pd.to_numeric(X['Accept'], errors='coerce').fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()\n",
    "X[\"Method_enc\"] = lb_make.fit_transform(X[\"Method\"])\n",
    "X[\"host_enc\"] =lb_make.fit_transform(X[\"host\"])\n",
    "X[\"Accept_enc\"] =lb_make.fit_transform(X[\"Accept\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count_met = X[\"Method_enc\"].nunique()\n",
    "unique_count_host = X[\"host_enc\"].nunique()\n",
    "unique_count_acc = X[\"Accept_enc\"].nunique()\n",
    "\n",
    "\n",
    "print(f\"Number of unique values for 'Method_enc': {unique_count_met}\")\n",
    "print(f\"Number of unique values for 'host_enc': {unique_count_host}\")\n",
    "print(f\"Number of unique values for 'Accept_enc': {unique_count_acc}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_content(content,function):\n",
    "    if pd.isna(content):\n",
    "        return 0\n",
    "    elif isinstance(content, str):\n",
    "        return function(content)\n",
    "\n",
    "#\"\"\"\n",
    "#                'count_dot_content','count_dir_content','count_embed_domain_content','count%_content','count?_content',\n",
    " #               'count-_content','count=_content','hostname_length_content','sus_content','count_digits_content',\n",
    "  #              'count_letters_content','content_length','number_of_parameters_content','number_of_fragments_content',\n",
    "   #             'is_encoded_content','special_count_content','unusual_character_ratio_content'\n",
    "    #            ]\"\"\"\n",
    "\n",
    "X['count_dot_content'] = X['content'].apply(apply_to_content, function=count_dot)\n",
    "X['count_dir_content'] = X['content'].apply(apply_to_content, function=no_of_dir)\n",
    "X['count_embed_domain_content'] = X['content'].apply(apply_to_content, function=no_of_embed)\n",
    "X['count%_content'] = X['content'].apply(apply_to_content, function=count_per)\n",
    "X['count?_content'] = X['content'].apply(apply_to_content, function=count_ques)\n",
    "X['count-_content'] = X['content'].apply(apply_to_content, function=count_hyphen)\n",
    "X['count=_content'] = X['content'].apply(apply_to_content, function=count_equal)\n",
    "X['content_length'] = X['content'].apply(apply_to_content, function=url_length)\n",
    "X['sus_content'] = X['content'].apply(apply_to_content, function=suspicious_words)\n",
    "X['count_digits_content'] = X['content'].apply(apply_to_content, function=digit_count)\n",
    "X['count_letters_content'] = X['content'].apply(apply_to_content, function=letter_count)\n",
    "X['special_count_content'] = X['content'].apply(apply_to_content, function=count_special_characters)\n",
    "X['is_encoded_content'] = X['content'].apply(apply_to_content, function=is_encoded)\n",
    "#X['unusual_character_ratio_content'] = X['content'].apply(apply_to_content, function=unusual_character_ratio)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the features and class variable for plotting\n",
    "new_content_features = ['count_dot_content', 'count_dir_content', 'count_embed_domain_content', 'count%_content', 'count?_content',\n",
    "                        'count-_content', 'count=_content', 'sus_content', 'count_digits_content',\n",
    "                        'count_letters_content', 'content_length', 'is_encoded_content', 'special_count_content']\n",
    "\n",
    "# Create a DataFrame with the selected features\n",
    "selected_features_df = X[new_content_features]\n",
    "\n",
    "for feature_name in selected_features_df.columns:\n",
    "    if feature_name in X.columns:\n",
    "        unique_count = selected_features_df[feature_name].nunique()\n",
    "        print(f\"Number of unique values for {feature_name}: {unique_count}\")\n",
    "    else:\n",
    "        print(f\"Column '{feature_name}' does not exist in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building the final dataset to use for the classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['count_dot_url', 'count_dir_url', 'count_embed_domain_url', 'count-http',\n",
    "                'count%_url', 'count?_url', 'count-_url', 'count=_url', 'url_length', 'hostname_length_url',\n",
    "                'sus_url', 'count-digits_url', 'count-letters_url', 'number_of_parameters_url',\n",
    "                'is_encoded_url','special_count_url','unusual_character_ratio_url',\n",
    "                 #method\n",
    "                'Method_enc',\n",
    "                #content\n",
    "                'count_dot_content','count%_content',\n",
    "                 'count-_content','count=_content','sus_content','count_digits_content',\n",
    "                  'count_letters_content','content_length',\n",
    "               'is_encoded_content','special_count_content']\n",
    "print(X[labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=X['classification']\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('computing...)')\n",
    "#split dataset in test and train \n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split(X[labels], y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RANDOM FOREST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(random_state=1000)\n",
    "print('Computing....')\n",
    "# Fit the model\n",
    "random_forest_model.fit(x_tr,y_tr)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_predictions= random_forest_model.predict(x_ts)\n",
    "print('MAE', mean_absolute_error(y_ts, RT_predictions))\n",
    "print(\"Accuracy\", accuracy_score(y_ts, RT_predictions))\n",
    "print(\"Precision\", precision_score(y_ts, RT_predictions, average='weighted', labels=np.unique(RT_predictions)))\n",
    "print(\"Recall\", recall_score(y_ts, RT_predictions, average='weighted', labels=np.unique(RT_predictions)))\n",
    "print(\"F1\", f1_score(y_ts, RT_predictions, average='weighted', labels=np.unique(RT_predictions)))\n",
    "print(\"ROC AUC\", roc_auc_score(y_ts, RT_predictions, average='weighted', labels=np.unique(RT_predictions)))\n",
    "error_rt = (RT_predictions != y_ts).mean()\n",
    "print(\"Test error: {:.1%}\".format(error_rt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_tr.unique())\n",
    "print(y_tr.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts = x_ts.reset_index(drop=True)\n",
    "y_ts = y_ts.reset_index(drop=True)\n",
    "\n",
    "for k in range(np.unique(y_ts).size):\n",
    "    print('mean of class ' + str(k) + ':\\n', x_ts[y_ts == k].mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_ts, RT_predictions, target_names = ['Normal (class 0)','Anomalous (class 1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label = ['Normal', 'Anomalous']\n",
    "cm = confusion_matrix(y_ts, RT_predictions)\n",
    "cm = pd.DataFrame(cm, index=['0', '1'], columns=['0', '1'])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, cmap=\"Blues\", linecolor='black', linewidth=1, annot=True, fmt='', xticklabels=label, yticklabels=label)\n",
    "plt.title(\"Random Forest\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-NEAREST NEIGHBOR**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#knn_model = KNeighborsClassifier()\n",
    "\n",
    "#param_grid = {'n_neighbors': [3, 5, 7, 9,10,11, 13]}\n",
    "\n",
    "#grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "#grid_search.fit(x_tr, y_tr)\n",
    "\n",
    "#best_n_neighbors = grid_search.best_params_['n_neighbors']\n",
    "#print(\"Best n_neighbors:\", best_n_neighbors)\n",
    "\n",
    "#final_model = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
    "#final_model.fit(x_tr, y_tr)\n",
    "\n",
    "#knn_predictions = final_model.predict(x_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best n_neighbors: 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = KNeighborsClassifier(n_neighbors=9)\n",
    "final_model.fit(x_tr, y_tr)\n",
    "knn_predictions = final_model.predict(x_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE', mean_absolute_error(y_ts, knn_predictions))\n",
    "print(\"Accuracy\", accuracy_score(y_ts, knn_predictions))\n",
    "print(\"Precision\", precision_score(y_ts, knn_predictions, average='weighted', labels=np.unique(knn_predictions)))\n",
    "print(\"Recall\", recall_score(y_ts, knn_predictions, average='weighted', labels=np.unique(knn_predictions)))\n",
    "print(\"F1\", f1_score(y_ts, knn_predictions, average='weighted', labels=np.unique(knn_predictions)))\n",
    "print(\"ROC AUC\", roc_auc_score(y_ts, knn_predictions, average='weighted', labels=np.unique(knn_predictions)))\n",
    "error_knn = (knn_predictions != y_ts).mean()\n",
    "print(\"Test error: {:.1%}\".format(error_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_ts,knn_predictions)\n",
    "cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.title(\"KN Neighbors\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DECISION TREE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier(random_state=2)\n",
    "print('Computing....')\n",
    "DT_model.fit(x_tr,y_tr)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_predictions= DT_model.predict(x_ts)\n",
    "print('MAE', mean_absolute_error(y_ts, DT_predictions))\n",
    "print(\"Accuracy\", accuracy_score(y_ts, DT_predictions))\n",
    "print(\"Precision\", precision_score(y_ts, DT_predictions, average='weighted', labels=np.unique(DT_predictions)))\n",
    "print(\"Recall\", recall_score(y_ts, DT_predictions, average='weighted', labels=np.unique(DT_predictions)))\n",
    "print(\"F1\", f1_score(y_ts, DT_predictions, average='weighted', labels=np.unique(DT_predictions)))\n",
    "print(\"ROC AUC\", roc_auc_score(y_ts, DT_predictions, average='weighted', labels=np.unique(DT_predictions)))\n",
    "error_dt = (DT_predictions != y_ts).mean()\n",
    "print(\"Test error: {:.1%}\".format(error_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_ts,DT_predictions)\n",
    "cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression(random_state = 42, max_iter = 1000)\n",
    "print('Computing....')\n",
    "LR_model.fit(x_tr,y_tr)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_predictions= LR_model.predict(x_ts)\n",
    "print('MAE', mean_absolute_error(y_ts, LR_predictions))\n",
    "print(\"Accuracy\", accuracy_score(y_ts, LR_predictions))\n",
    "print(\"Precision\", precision_score(y_ts, LR_predictions, average='weighted', labels=np.unique(LR_predictions)))\n",
    "print(\"Recall\", recall_score(y_ts, LR_predictions, average='weighted', labels=np.unique(LR_predictions)))\n",
    "print(\"F1\", f1_score(y_ts, LR_predictions, average='weighted', labels=np.unique(LR_predictions)))\n",
    "print(\"ROC AUC\", roc_auc_score(y_ts,LR_predictions, average='weighted', labels=np.unique(LR_predictions)))\n",
    "error_lr = (LR_predictions != y_ts).mean()\n",
    "print(\"Test error: {:.1%}\".format(error_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_ts,LR_predictions)\n",
    "cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n",
    "plt.title(\"Logistic Regression\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model = SVC()\n",
    "print('Computing....') \n",
    "SVC_model.fit(x_tr,y_tr)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_predictions= SVC_model.predict(x_ts)\n",
    "print('MAE', mean_absolute_error(y_ts, SVC_predictions))\n",
    "print(\"Accuracy\", accuracy_score(y_ts, SVC_predictions))\n",
    "print(\"Precision\", precision_score(y_ts, SVC_predictions, average='weighted', labels=np.unique(SVC_predictions)))\n",
    "print(\"Recall\", recall_score(y_ts, SVC_predictions, average='weighted', labels=np.unique(SVC_predictions)))\n",
    "print(\"F1\", f1_score(y_ts, SVC_predictions, average='weighted', labels=np.unique(SVC_predictions)))\n",
    "print(\"ROC AUC\", roc_auc_score(y_ts,SVC_predictions, average='weighted', labels=np.unique(SVC_predictions)))\n",
    "error_svc = (SVC_predictions != y_ts).mean()\n",
    "print(\"Test error: {:.1%}\".format(error_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_ts,SVC_predictions)\n",
    "cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n",
    "plt.title(\"SVC\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naïves Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = GaussianNB ()\n",
    "print('Computing....')\n",
    "NB_model.fit(x_tr,y_tr)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_predictions= NB_model.predict(x_ts)\n",
    "print('MAE', mean_absolute_error(y_ts, NB_predictions))\n",
    "print(\"Accuracy\", accuracy_score(y_ts, NB_predictions))\n",
    "print(\"Precision\", precision_score(y_ts, NB_predictions, average='weighted', labels=np.unique(NB_predictions)))\n",
    "print(\"Recall\", recall_score(y_ts, NB_predictions, average='weighted', labels=np.unique(NB_predictions)))\n",
    "print(\"F1\", f1_score(y_ts, NB_predictions, average='weighted', labels=np.unique(NB_predictions)))\n",
    "print(\"ROC AUC\", roc_auc_score(y_ts,NB_predictions, average='weighted', labels=np.unique(NB_predictions)))\n",
    "error_nb = (NB_predictions != y_ts).mean()\n",
    "print(\"Test error: {:.1%}\".format(error_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_ts,NB_predictions)\n",
    "cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n",
    "plt.title(\"NB\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Network(RNN)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming x_tr, y_tr, x_ts, y_ts are your training and testing data\n",
    "# Ensure they are numpy arrays or can be converted to numpy arrays\n",
    "# Ensure that y_tr and y_ts are properly encoded (binary labels)\n",
    "\n",
    "# Example label encoding for binary classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_tr_encoded = label_encoder.fit_transform(y_tr)\n",
    "y_ts_encoded = label_encoder.transform(y_ts)\n",
    "\n",
    "# Reshape x_tr and x_ts if necessary (assuming they are 2D arrays)\n",
    "# Add this if x_tr and x_ts are 1D arrays: x_tr = x_tr.reshape(-1, 1)\n",
    "\n",
    "# Initialize RNN model\n",
    "RNN_model = Sequential()\n",
    "RNN_model.add(SimpleRNN(50, input_shape=(x_tr.shape[1], 1), activation='relu'))\n",
    "RNN_model.add(Dense(units=1, activation='sigmoid'))  # Adjust units and activation based on your task\n",
    "\n",
    "# Compile the model\n",
    "RNN_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "RNN_model.fit(x_tr, y_tr_encoded, epochs=50, batch_size=32, validation_data=(x_ts, y_ts_encoded))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = RNN_model.evaluate(x_ts, y_ts_encoded)[1]\n",
    "\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial Neural Network(ANN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming x_tr, y_tr, x_ts, y_ts are your training and testing data\n",
    "# Ensure they are numpy arrays or can be converted to numpy arrays\n",
    "# Ensure that y_tr and y_ts are properly encoded (binary labels)\n",
    "\n",
    "# Example label encoding for binary classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_tr_encoded = label_encoder.fit_transform(y_tr)\n",
    "y_ts_encoded = label_encoder.transform(y_ts)\n",
    "\n",
    "# Initialize ANN model\n",
    "ANN_model = Sequential()\n",
    "ANN_model.add(Dense(50, input_shape=(x_tr.shape[1],), activation='relu'))\n",
    "ANN_model.add(Dense(units=1, activation='sigmoid'))  # Adjust units and activation based on your task\n",
    "\n",
    "# Compile the model\n",
    "ANN_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "ANN_model.fit(x_tr, y_tr_encoded, epochs=30, batch_size=32, validation_data=(x_ts, y_ts_encoded))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = ANN_model.evaluate(x_ts, y_ts_encoded)[1]\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network(CNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming x_tr, y_tr, x_ts, y_ts are your training and testing data DataFrames\n",
    "# Convert DataFrames to numpy arrays\n",
    "x_tr = x_tr.to_numpy()\n",
    "x_ts = x_ts.to_numpy()\n",
    "\n",
    "# Example label encoding for binary classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_tr_encoded = label_encoder.fit_transform(y_tr)\n",
    "y_ts_encoded = label_encoder.transform(y_ts)\n",
    "\n",
    "# Reshape x_tr and x_ts to match the input shape expected by the model\n",
    "x_tr = x_tr.reshape(x_tr.shape[0], -1)  # Flattens the input to a 1D array\n",
    "x_ts = x_ts.reshape(x_ts.shape[0], -1)  # Flattens the input to a 1D array\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(x_tr.shape[1],)))  # Flattens the input\n",
    "model.add(Dense(64, activation='relu'))  # Add a dense layer with 64 neurons and ReLU activation\n",
    "model.add(Dense(32, activation='relu'))  # Add another dense layer with 32 neurons and ReLU activation\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_tr, y_tr_encoded, epochs=30, batch_size=32, validation_data=(x_ts, y_ts_encoded))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(x_ts, y_ts_encoded)[1]\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long Short-Term Memory(LSTM)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming x_tr, y_tr, x_ts, y_ts are your training and testing data\n",
    "# Ensure they are numpy arrays or can be converted to numpy arrays\n",
    "# Ensure that y_tr and y_ts are properly encoded (binary labels)\n",
    "\n",
    "# Example label encoding for binary classification\n",
    "label_encoder = LabelEncoder()\n",
    "y_tr_encoded = label_encoder.fit_transform(y_tr)\n",
    "y_ts_encoded = label_encoder.transform(y_ts)\n",
    "\n",
    "# Reshape x_tr and x_ts if necessary (assuming they are 2D arrays)\n",
    "# Add this if x_tr and x_ts are 1D arrays: x_tr = x_tr.reshape(-1, 1)\n",
    "\n",
    "# Initialize LSTM model\n",
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(LSTM(50, input_shape=(x_tr.shape[1], 1), activation='relu'))\n",
    "LSTM_model.add(Dense(units=1, activation='sigmoid'))  # Adjust units and activation based on your task\n",
    "\n",
    "# Compile the model\n",
    "LSTM_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "LSTM_model.fit(x_tr, y_tr_encoded, epochs=25, batch_size=32, validation_data=(x_ts, y_ts_encoded))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = LSTM_model.evaluate(x_ts, y_ts_encoded)[1]\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RANKING THE TRAINED MODELS ON THE MAE VALUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_preds = [RT_predictions, knn_predictions, DT_predictions, LR_predictions, SVC_predictions, NB_predictions]\n",
    "model_names = ['Random Forest', 'K-Nearest Neighbors', 'Decision Tree', 'Logistic Regression', 'SVC', 'NB']\n",
    "\n",
    "# Function for comparing different models\n",
    "def score_model(model_preds, y_ts):\n",
    "    return accuracy_score(y_ts, model_preds)\n",
    "\n",
    "# Calculate Accuracy for each model\n",
    "acc_score = []\n",
    "for i in range(len(model_names)):\n",
    "    acc = score_model(model_preds[i], y_ts)\n",
    "    acc_score.append((model_names[i], acc))\n",
    "\n",
    "acc_scores_sorted = sorted(acc_score, key=lambda x: x[1], reverse= True)\n",
    "target_range = y_ts.max() - y_ts.min()\n",
    "\n",
    "# Print ranked model names, MAE scores, and error percentages\n",
    "# for i, (model_name, acc) in enumerate(acc_scores_sorted):\n",
    "#     error_percent = (mae / target_range) * 100  # Calculate error percentage\n",
    "#     print(\"Rank %d: %s - Accuracy: %.4f - : %.2f%%\" % (i+1, model_name, acc, error_percent))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = [RT_predictions, knn_predictions, DT_predictions, LR_predictions, SVC_predictions, NB_predictions]\n",
    "model_names = ['Random Forest', 'K-Nearest Neighbors', 'Decision Tree', 'Logistic Regression','SVC', 'NB']\n",
    "\n",
    "# Calculate Accuracy for each model\n",
    "acc_score = []\n",
    "for i in range(len(model_names)):\n",
    "    acc = score_model(model_preds[i], y_ts)\n",
    "    acc_score.append((model_names[i], acc))\n",
    "\n",
    "acc_scores_sorted = sorted(acc_score, key=lambda x: x[1], reverse= True)\n",
    "target_range = y_ts.max() - y_ts.min()\n",
    "\n",
    "# Print ranked model names, Accuracy scores, and Accuracy percentages\n",
    "for i, (model_name, acc) in enumerate(acc_scores_sorted):\n",
    "    error_percent = (acc / target_range) * 100  # Calculate error percentage\n",
    "    print(\"Rank %d: %s - ACC: %.4f - Accuracy: %.2f%%\" % (i+1, model_name, acc, error_percent))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 378374,
     "sourceId": 734120,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
