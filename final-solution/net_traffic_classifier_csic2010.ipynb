{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Anomaly detection - HTTP requests - CSIC dataset 2010\n","\n","## Task\n","The primary objective is to develop a classifier capable of identifying malicious HTTP requests by training on normal traffic data and evaluating both normal and anomalous test data. The dataset is structured as follows:\n","\n"," * Normal Traffic (Train)\n"," * Normal Traffic (Test)\n"," * Anomalous Traffic (Test)\n","\n","Although the dataset is designed for unsupervised learning, supervised learning techniques can be applied by combining normal and anomalous data into a labeled dataset. This allows for direct classification using any preferred machine learning model.\n","\n","## Dataset\n","The dataset contains the generated traffic targeted to an e-commerce web\n","application. It is an automatically generated dataset that contains 36,000 normal\n","requests and more than 25,000 anomalous requests (i.e., web attacks)."]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'eli5'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Feature importance and explainability\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PermutationImportance\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# URL processing\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eli5'"]}],"source":["# Core libraries\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import re\n","import math\n","\n","# Feature importance and explainability\n","import eli5\n","from eli5.sklearn import PermutationImportance\n","\n","# URL processing\n","from urllib.parse import urlparse\n","\n","# Scikit-learn preprocessing and model evaluation\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    precision_score, accuracy_score, recall_score, f1_score, \n","    roc_auc_score, mean_absolute_error, confusion_matrix, \n","    classification_report\n",")\n","\n","# Classifiers\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import SGDClassifier, LogisticRegression\n","from sklearn.svm import LinearSVC, SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from xgboost import XGBClassifier\n","\n","# Model selection and hyperparameter tuning\n","from sklearn.model_selection import GridSearchCV, StratifiedKFold\n","\n","# Deep Neural Networks (DNN) with TensorFlow/Keras\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","\n","# Explainability tools\n","from sklearn.inspection import PartialDependenceDisplay"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["csic_dataset/normalTrafficTraining.txt.gz\n","csic_dataset/anomalousTrafficTest.txt\n","csic_dataset/.DS_Store\n","csic_dataset/normalTrafficTest.txt.gz\n","csic_dataset/normalTrafficTraining.txt\n","csic_dataset/anomalousTrafficTest.txt.gz\n","csic_dataset/normalTrafficTest.txt\n","Done!\n"]}],"source":["import os\n","\n","# List all files under the specified directory (in this case, '/csic_dataset')\n","for dirname, _, filenames in os.walk('csic_dataset/'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Confirmation that the operation has completed\n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["## Data Import"]},{"cell_type":"markdown","metadata":{},"source":["Now, since the assignment specified that the model should be trained only on the normalTrafficTraining.txt file, we will first try only with this."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 1 fields in line 5, saw 7\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m csic_filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsic_dataset/normalTrafficTraining.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m csic_data\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsic_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n","File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 5, saw 7\n"]}],"source":["csic_filepath='csic_dataset/normalTrafficTraining.txt'\n","data=pd.read_csv(csic_filepath)\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_features=data.shape[1]\n","n_samples=data.shape[0]\n","\n","print(\"Number of samples:\", n_samples)\n","print(\"Number of features:\", n_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# csic_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["csic_data.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["csic_data.columns\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Visualization**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.set_style('darkgrid')\n","sns.countplot(data=csic_data, x='Unnamed: 0')"]},{"cell_type":"markdown","metadata":{},"source":["Dropping samples with at least 1 NaN value will make to lose all the other Request Methods besides POST, this option is discarded since dropping data is not usually a good choice"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["csic_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["Visualizing URL format"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feature_names=[ 'Unnamed: 0','Method', 'User-Agent', 'Pragma', 'Cache-Control',\n","       'Accept', 'Accept-encoding', 'Accept-charset', 'language', 'host',\n","       'cookie', 'content-type', 'connection', 'lenght', 'content','classification',\n","        'URL']\n","\n","X=csic_data[feature_names]\n","print(X)"]},{"cell_type":"markdown","metadata":{},"source":["# **Removing not discriminatory features**\n","\n","**Enumerating unique values for each feature**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Removing not discriminatory Features and making some adjustments on feature names\n","X = X.rename(columns={'Unnamed: 0': 'Class'})\n","X = X.rename(columns={'lenght': 'content_length'})\n","\n","\n","feature_names=[ 'Class','Method','host','cookie','Accept', 'content_length', 'content','classification','URL']\n","\n","# Print the remaining data\n","X = X[feature_names]\n","print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y=X.Class\n","print(y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["size=X.shape[1]\n","# Get list of categorical variables\n","s = (X.dtypes == 'object')\n","object_cols = list(s[s].index)\n","\n","print(\"Categorical variables:\")\n","print(object_cols)"]},{"cell_type":"markdown","metadata":{},"source":["Load Models"]},{"cell_type":"markdown","metadata":{},"source":["# **Pre-processing on the feature: Content Length**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(X.content_length)"]},{"cell_type":"markdown","metadata":{},"source":["Operations on the 'content_lenght' feature"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#replace NaN values with 0\n","#removing the 'Content-Lenght' string and keeping only the numerical value\n","\n","X['content_length'] = X['content_length'].astype(str)\n","X['content_length'] = X['content_length'].str.extract(r'(\\d+)')\n","X['content_length'] = pd.to_numeric(X['content_length'], errors='coerce').fillna(0)\n","print(X.content_length)\n"]},{"cell_type":"markdown","metadata":{},"source":["GET methods have the content_length set to 0 since they where all NaN (this method does not have to provide content)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["filtered_length = X.loc[X['Method'] == 'GET', 'content_length']\n","print(filtered_length)\n"]},{"cell_type":"markdown","metadata":{},"source":["# URL PRE-PROCESSING"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["url_counts = X['URL'].value_counts()\n","most_common_urls = url_counts.head(10)  # Extract the top 10 most common strings\n","\n","print(\"Most common URLs:\")\n","for i, (url, count) in enumerate(most_common_urls.items(), 1):\n","    print(f\"{i}. URL: {url} - Count: {count}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["**Utils for URL/Content pre-processing**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def count_dot(url):\n","    count_dot = url.count('.')\n","    return count_dot\n","\n","\n","def no_of_dir(url):\n","    urldir = urlparse(url).path\n","    return urldir.count('/')\n","\n","def no_of_embed(url):\n","    urldir = urlparse(url).path\n","    return urldir.count('//')\n","\n","def shortening_service(url):\n","    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n","                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n","                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n","                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n","                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n","                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n","                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n","                      'tr\\.im|link\\.zip\\.net',\n","                      url)\n","    if match:\n","        return 1\n","    else:\n","        return 0\n","\n","\n","def count_http(url):\n","    return url.count('http')\n","\n","def count_per(url):\n","    return url.count('%')\n","\n","def count_ques(url):\n","    return url.count('?')\n","\n","def count_hyphen(url):\n","    return url.count('-')\n","\n","\n","def count_equal(url):\n","    return url.count('=')\n","\n","\n","def url_length(url):\n","    return len(str(url))\n","\n","#Hostname Length\n","\n","def hostname_length(url):\n","    return len(urlparse(url).netloc)\n","\n","\n","import re\n","\n","def suspicious_words(url):\n","    score_map = {\n","        'error': 30,\n","        'errorMsg': 30,\n","        'id': 10,\n","        'errorID': 30,\n","        'SELECT': 50,\n","        'FROM': 50,\n","        'WHERE': 50,\n","        'DELETE': 50,\n","        'USERS': 50,\n","        'DROP': 50,\n","        'CREATE': 50,\n","        'INJECTED': 50,\n","        'TABLE': 50,\n","        'alert': 30,\n","        'javascript': 20,\n","        'cookie': 25,\n","        '--': 30,\n","        '.exe': 30,\n","        '.php': 20,\n","        '.js': 10,\n","        'admin': 10,\n","        'administrator': 10,\n","        '\\'': 30,\n","        'password': 15,\n","        'login': 15,\n","        'incorrect': 20,\n","        'pwd': 15,\n","        'tamper': 25,\n","        'vaciar': 20,\n","        'carrito': 25,\n","        'wait': 30,\n","        'delay': 35,\n","        'set': 20,\n","        'steal': 35,\n","        'hacker': 35,\n","        'proxy': 35,\n","        'location': 30,\n","        'document.cookie': 40,\n","        'document': 20,\n","        'set-cookie': 40,\n","        'create': 40,\n","        'cmd': 40,\n","        'dir': 30,\n","        'shell': 40,\n","        'reverse': 30,\n","        'bin': 20,\n","        'cookiesteal': 40,\n","        'LIKE': 30,\n","        'UNION': 35,\n","        'include': 30,\n","        'file': 20,\n","        'tmp': 25,\n","        'ssh': 40,\n","        'exec': 30,\n","        'cat': 25,\n","        'etc': 30,\n","        'fetch': 25,\n","        'eval': 30,\n","        'wait': 30,\n","        'malware': 45,\n","        'ransomware': 45,\n","        'phishing': 45,\n","        'exploit': 45,\n","        'virus': 45,\n","        'trojan': 45,\n","        'backdoor': 45,\n","        'spyware': 45,\n","        'rootkit': 45,\n","        'credential': 30,\n","        'inject': 30,\n","        'script': 25,\n","        'iframe': 25,\n","        'src=': 25,\n","        'onerror': 30,\n","        'prompt': 20,\n","        'confirm': 20,\n","        'eval': 25,\n","        'expression': 30,\n","        'function\\(': 20,\n","        'xmlhttprequest': 30,\n","        'xhr': 20,\n","        'window.': 20,\n","        'document.': 20,\n","        'cookie': 25,\n","        'click': 15,\n","        'mouseover': 15,\n","        'onload': 20,\n","        'onunload': 20,\n","    }\n","\n","    matches = re.findall(r'(?i)' + '|'.join(score_map.keys()), url)\n","\n","    total_score = sum(score_map.get(match.lower(), 0) for match in matches)\n","    return total_score\n","\n","\n","def digit_count(url):\n","    digits = 0\n","    for i in url:\n","        if i.isnumeric():\n","            digits = digits + 1\n","    return digits\n","\n","def letter_count(url):\n","    letters = 0\n","    for i in url:\n","        if i.isalpha():\n","            letters += 1\n","    return letters\n","\n","def count_special_characters(url):\n","    special_characters = re.sub(r'[a-zA-Z0-9\\s]', '', url)\n","    count = len(special_characters)\n","    return count\n","\n","\n","# Number of Parameters in URL\n","def number_of_parameters(url):\n","    params = urlparse(url).query\n","    return 0 if params == '' else len(params.split('&'))\n","\n","# Number of Fragments in URL\n","def number_of_fragments(url):\n","    frags = urlparse(url).fragment\n","    return len(frags.split('#')) - 1 if frags == '' else 0\n","\n","# URL is Encoded\n","def is_encoded(url):\n","    return int('%' in url.lower())\n","\n","\n","def unusual_character_ratio(url):\n","    total_characters = len(url)\n","    unusual_characters = re.sub(r'[a-zA-Z0-9\\s\\-._]', '', url)\n","    unusual_count = len(unusual_characters)\n","    ratio = unusual_count / total_characters if total_characters > 0 else 0\n","    return ratio\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X['count_dot_url'] = X['URL'].apply(count_dot)\n","X['count_dir_url'] = X['URL'].apply(no_of_dir)\n","X['count_embed_domain_url'] = X['URL'].apply(no_of_embed)\n","X['short_url'] = X['URL'].apply(shortening_service)\n","X['count-http'] = X['URL'].apply(count_http)\n","X['count%_url'] = X['URL'].apply(count_per)\n","X['count?_url'] = X['URL'].apply(count_ques)\n","X['count-_url'] = X['URL'].apply(count_hyphen)\n","X['count=_url'] = X['URL'].apply(count_equal)\n","X['hostname_length_url'] = X['URL'].apply(hostname_length)\n","X['sus_url'] = X['URL'].apply(suspicious_words)\n","X['count-digits_url'] = X['URL'].apply(digit_count)\n","X['count-letters_url'] = X['URL'].apply(letter_count)\n","X['url_length'] = X['URL'].apply(url_length)\n","X['number_of_parameters_url'] = X['URL'].apply(number_of_parameters)\n","X['number_of_fragments_url'] = X['URL'].apply(number_of_fragments)\n","X['is_encoded_url'] = X['URL'].apply(is_encoded)\n","X['special_count_url'] = X['URL'].apply(count_special_characters)\n","X['unusual_character_ratio_url'] = X['URL'].apply(unusual_character_ratio)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Select the features and class variable for plotting\n","new_features = ['count_dot_url', 'count_dir_url', 'count_embed_domain_url', 'count-http',\n","                'count%_url', 'count?_url', 'count-_url', 'count=_url', 'url_length', 'hostname_length_url',\n","                'sus_url', 'count-digits_url', 'count-letters_url', 'number_of_parameters_url',\n","                'number_of_fragments_url', 'is_encoded_url','special_count_url','unusual_character_ratio_url']\n","\n","# Create a DataFrame with the selected features\n","set = X[new_features]\n","\n","for new_feature in X.columns:\n","    if new_feature in X.columns:\n","        unique_count = X[new_feature].nunique()\n","        print(f\"Number of unique values for {new_feature}: {unique_count}\")\n","    else:\n","        print(f\"Column '{new_feature}' does not exist in the DataFrame.\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Removing Cookies as feature\n"," **cookies are unique for each sample, this feature cannot be used as discriminant**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["unique_count = X['cookie'].nunique()\n","print(f\"Count of unique values in 'cookie': {unique_count}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Encoding categorical features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X['Accept'] = X['Accept'].astype(str)\n","X['Accept'] = X['Accept'].str.extract(r'(\\d+)')\n","X['Accept'] = pd.to_numeric(X['Accept'], errors='coerce').fillna(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lb_make = LabelEncoder()\n","X[\"Method_enc\"] = lb_make.fit_transform(X[\"Method\"])\n","X[\"host_enc\"] =lb_make.fit_transform(X[\"host\"])\n","X[\"Accept_enc\"] =lb_make.fit_transform(X[\"Accept\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["unique_count_met = X[\"Method_enc\"].nunique()\n","unique_count_host = X[\"host_enc\"].nunique()\n","unique_count_acc = X[\"Accept_enc\"].nunique()\n","\n","\n","print(f\"Number of unique values for 'Method_enc': {unique_count_met}\")\n","print(f\"Number of unique values for 'host_enc': {unique_count_host}\")\n","print(f\"Number of unique values for 'Accept_enc': {unique_count_acc}\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X.head()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def apply_to_content(content,function):\n","    if pd.isna(content):\n","        return 0\n","    elif isinstance(content, str):\n","        return function(content)\n","\n","#\"\"\"\n","#                'count_dot_content','count_dir_content','count_embed_domain_content','count%_content','count?_content',\n"," #               'count-_content','count=_content','hostname_length_content','sus_content','count_digits_content',\n","  #              'count_letters_content','content_length','number_of_parameters_content','number_of_fragments_content',\n","   #             'is_encoded_content','special_count_content','unusual_character_ratio_content'\n","    #            ]\"\"\"\n","\n","X['count_dot_content'] = X['content'].apply(apply_to_content, function=count_dot)\n","X['count_dir_content'] = X['content'].apply(apply_to_content, function=no_of_dir)\n","X['count_embed_domain_content'] = X['content'].apply(apply_to_content, function=no_of_embed)\n","X['count%_content'] = X['content'].apply(apply_to_content, function=count_per)\n","X['count?_content'] = X['content'].apply(apply_to_content, function=count_ques)\n","X['count-_content'] = X['content'].apply(apply_to_content, function=count_hyphen)\n","X['count=_content'] = X['content'].apply(apply_to_content, function=count_equal)\n","X['content_length'] = X['content'].apply(apply_to_content, function=url_length)\n","X['sus_content'] = X['content'].apply(apply_to_content, function=suspicious_words)\n","X['count_digits_content'] = X['content'].apply(apply_to_content, function=digit_count)\n","X['count_letters_content'] = X['content'].apply(apply_to_content, function=letter_count)\n","X['special_count_content'] = X['content'].apply(apply_to_content, function=count_special_characters)\n","X['is_encoded_content'] = X['content'].apply(apply_to_content, function=is_encoded)\n","#X['unusual_character_ratio_content'] = X['content'].apply(apply_to_content, function=unusual_character_ratio)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Select the features and class variable for plotting\n","new_content_features = ['count_dot_content', 'count_dir_content', 'count_embed_domain_content', 'count%_content', 'count?_content',\n","                        'count-_content', 'count=_content', 'sus_content', 'count_digits_content',\n","                        'count_letters_content', 'content_length', 'is_encoded_content', 'special_count_content']\n","\n","# Create a DataFrame with the selected features\n","selected_features_df = X[new_content_features]\n","\n","for feature_name in selected_features_df.columns:\n","    if feature_name in X.columns:\n","        unique_count = selected_features_df[feature_name].nunique()\n","        print(f\"Number of unique values for {feature_name}: {unique_count}\")\n","    else:\n","        print(f\"Column '{feature_name}' does not exist in the DataFrame.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X.columns"]},{"cell_type":"markdown","metadata":{},"source":["# **Building the final dataset to use for the classification**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["labels=['count_dot_url', 'count_dir_url', 'count_embed_domain_url', 'count-http',\n","                'count%_url', 'count?_url', 'count-_url', 'count=_url', 'url_length', 'hostname_length_url',\n","                'sus_url', 'count-digits_url', 'count-letters_url', 'number_of_parameters_url',\n","                'is_encoded_url','special_count_url','unusual_character_ratio_url',\n","                 #method\n","                'Method_enc',\n","                #content\n","                'count_dot_content','count%_content',\n","                 'count-_content','count=_content','sus_content','count_digits_content',\n","                  'count_letters_content','content_length',\n","               'is_encoded_content','special_count_content']\n","print(X[labels])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y=X['classification']\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('computing...)')\n","#split dataset in test and train \n","x_tr, x_ts, y_tr, y_ts = train_test_split(X[labels], y, test_size=0.3, random_state=0)\n","\n","\n","print('Done!')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_tr.head(5)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_tr.tail(5)"]},{"cell_type":"markdown","metadata":{},"source":["# Classifiers"]},{"cell_type":"markdown","metadata":{},"source":["**RANDOM FOREST**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["random_forest_model = RandomForestClassifier(random_state=1000)\n","print('Computing....')\n","# Fit the model\n","random_forest_model.fit(x_tr,y_tr)\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["RT_predictions= random_forest_model.predict(x_ts)\n","print('MAE', mean_absolute_error(y_ts, RT_predictions))\n","print(\"Accuracy\", accuracy_score(y_ts, RT_predictions))\n","print(\"Precision\", precision_score(y_ts, RT_predictions, average='weighted', labels=np.unique(RT_predictions)))\n","print(\"Recall\", recall_score(y_ts, RT_predictions, average='weighted', labels=np.unique(RT_predictions)))\n","print(\"F1\", f1_score(y_ts, RT_predictions, average='weighted', labels=np.unique(RT_predictions)))\n","print(\"ROC AUC\", roc_auc_score(y_ts, RT_predictions, average='weighted', labels=np.unique(RT_predictions)))\n","error_rt = (RT_predictions != y_ts).mean()\n","print(\"Test error: {:.1%}\".format(error_rt))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(y_tr.unique())\n","print(y_tr.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_ts = x_ts.reset_index(drop=True)\n","y_ts = y_ts.reset_index(drop=True)\n","\n","for k in range(np.unique(y_ts).size):\n","    print('mean of class ' + str(k) + ':\\n', x_ts[y_ts == k].mean(axis=0))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(classification_report(y_ts, RT_predictions, target_names = ['Normal (class 0)','Anomalous (class 1)']))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","label = ['Normal', 'Anomalous']\n","cm = confusion_matrix(y_ts, RT_predictions)\n","cm = pd.DataFrame(cm, index=['0', '1'], columns=['0', '1'])\n","\n","plt.figure(figsize=(10, 10))\n","sns.heatmap(cm, cmap=\"Blues\", linecolor='black', linewidth=1, annot=True, fmt='', xticklabels=label, yticklabels=label)\n","plt.title(\"Random Forest\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**K-NEAREST NEIGHBOR**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","#knn_model = KNeighborsClassifier()\n","\n","#param_grid = {'n_neighbors': [3, 5, 7, 9,10,11, 13]}\n","\n","#grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n","#grid_search.fit(x_tr, y_tr)\n","\n","#best_n_neighbors = grid_search.best_params_['n_neighbors']\n","#print(\"Best n_neighbors:\", best_n_neighbors)\n","\n","#final_model = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n","#final_model.fit(x_tr, y_tr)\n","\n","#knn_predictions = final_model.predict(x_ts)"]},{"cell_type":"markdown","metadata":{},"source":["Best n_neighbors: 9\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_model = KNeighborsClassifier(n_neighbors=9)\n","final_model.fit(x_tr, y_tr)\n","knn_predictions = final_model.predict(x_ts)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('MAE', mean_absolute_error(y_ts, knn_predictions))\n","print(\"Accuracy\", accuracy_score(y_ts, knn_predictions))\n","print(\"Precision\", precision_score(y_ts, knn_predictions, average='weighted', labels=np.unique(knn_predictions)))\n","print(\"Recall\", recall_score(y_ts, knn_predictions, average='weighted', labels=np.unique(knn_predictions)))\n","print(\"F1\", f1_score(y_ts, knn_predictions, average='weighted', labels=np.unique(knn_predictions)))\n","print(\"ROC AUC\", roc_auc_score(y_ts, knn_predictions, average='weighted', labels=np.unique(knn_predictions)))\n","error_knn = (knn_predictions != y_ts).mean()\n","print(\"Test error: {:.1%}\".format(error_knn))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_ts,knn_predictions)\n","cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n","plt.figure(figsize = (10,10))\n","plt.title(\"KN Neighbors\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)\n"]},{"cell_type":"markdown","metadata":{},"source":["**DECISION TREE**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DT_model = DecisionTreeClassifier(random_state=2)\n","print('Computing....')\n","DT_model.fit(x_tr,y_tr)\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DT_predictions= DT_model.predict(x_ts)\n","print('MAE', mean_absolute_error(y_ts, DT_predictions))\n","print(\"Accuracy\", accuracy_score(y_ts, DT_predictions))\n","print(\"Precision\", precision_score(y_ts, DT_predictions, average='weighted', labels=np.unique(DT_predictions)))\n","print(\"Recall\", recall_score(y_ts, DT_predictions, average='weighted', labels=np.unique(DT_predictions)))\n","print(\"F1\", f1_score(y_ts, DT_predictions, average='weighted', labels=np.unique(DT_predictions)))\n","print(\"ROC AUC\", roc_auc_score(y_ts, DT_predictions, average='weighted', labels=np.unique(DT_predictions)))\n","error_dt = (DT_predictions != y_ts).mean()\n","print(\"Test error: {:.1%}\".format(error_dt))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_ts,DT_predictions)\n","cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n","plt.title(\"Decision Tree\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)"]},{"cell_type":"markdown","metadata":{},"source":["**Logistic Regression**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["LR_model = LogisticRegression(random_state = 42, max_iter = 1000)\n","print('Computing....')\n","LR_model.fit(x_tr,y_tr)\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["LR_predictions= LR_model.predict(x_ts)\n","print('MAE', mean_absolute_error(y_ts, LR_predictions))\n","print(\"Accuracy\", accuracy_score(y_ts, LR_predictions))\n","print(\"Precision\", precision_score(y_ts, LR_predictions, average='weighted', labels=np.unique(LR_predictions)))\n","print(\"Recall\", recall_score(y_ts, LR_predictions, average='weighted', labels=np.unique(LR_predictions)))\n","print(\"F1\", f1_score(y_ts, LR_predictions, average='weighted', labels=np.unique(LR_predictions)))\n","print(\"ROC AUC\", roc_auc_score(y_ts,LR_predictions, average='weighted', labels=np.unique(LR_predictions)))\n","error_lr = (LR_predictions != y_ts).mean()\n","print(\"Test error: {:.1%}\".format(error_lr))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_ts,LR_predictions)\n","cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n","plt.title(\"Logistic Regression\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)"]},{"cell_type":"markdown","metadata":{},"source":["**Support Vector Machine (SVM)**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["SVC_model = SVC()\n","print('Computing....') \n","SVC_model.fit(x_tr,y_tr)\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["SVC_predictions= SVC_model.predict(x_ts)\n","print('MAE', mean_absolute_error(y_ts, SVC_predictions))\n","print(\"Accuracy\", accuracy_score(y_ts, SVC_predictions))\n","print(\"Precision\", precision_score(y_ts, SVC_predictions, average='weighted', labels=np.unique(SVC_predictions)))\n","print(\"Recall\", recall_score(y_ts, SVC_predictions, average='weighted', labels=np.unique(SVC_predictions)))\n","print(\"F1\", f1_score(y_ts, SVC_predictions, average='weighted', labels=np.unique(SVC_predictions)))\n","print(\"ROC AUC\", roc_auc_score(y_ts,SVC_predictions, average='weighted', labels=np.unique(SVC_predictions)))\n","error_svc = (SVC_predictions != y_ts).mean()\n","print(\"Test error: {:.1%}\".format(error_svc))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_ts,SVC_predictions)\n","cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n","plt.title(\"SVC\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)"]},{"cell_type":"markdown","metadata":{},"source":["**Na√Øves Bayes**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NB_model = GaussianNB ()\n","print('Computing....')\n","NB_model.fit(x_tr,y_tr)\n","print('Done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NB_predictions= NB_model.predict(x_ts)\n","print('MAE', mean_absolute_error(y_ts, NB_predictions))\n","print(\"Accuracy\", accuracy_score(y_ts, NB_predictions))\n","print(\"Precision\", precision_score(y_ts, NB_predictions, average='weighted', labels=np.unique(NB_predictions)))\n","print(\"Recall\", recall_score(y_ts, NB_predictions, average='weighted', labels=np.unique(NB_predictions)))\n","print(\"F1\", f1_score(y_ts, NB_predictions, average='weighted', labels=np.unique(NB_predictions)))\n","print(\"ROC AUC\", roc_auc_score(y_ts,NB_predictions, average='weighted', labels=np.unique(NB_predictions)))\n","error_nb = (NB_predictions != y_ts).mean()\n","print(\"Test error: {:.1%}\".format(error_nb))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_ts,NB_predictions)\n","cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n","plt.title(\"NB\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = label,yticklabels = label)"]},{"cell_type":"markdown","metadata":{},"source":["**Recurrent Neural Network(RNN)** "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import SimpleRNN, Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# Assuming x_tr, y_tr, x_ts, y_ts are your training and testing data\n","# Ensure they are numpy arrays or can be converted to numpy arrays\n","# Ensure that y_tr and y_ts are properly encoded (binary labels)\n","\n","# Example label encoding for binary classification\n","label_encoder = LabelEncoder()\n","y_tr_encoded = label_encoder.fit_transform(y_tr)\n","y_ts_encoded = label_encoder.transform(y_ts)\n","\n","# Reshape x_tr and x_ts if necessary (assuming they are 2D arrays)\n","# Add this if x_tr and x_ts are 1D arrays: x_tr = x_tr.reshape(-1, 1)\n","\n","# Initialize RNN model\n","RNN_model = Sequential()\n","RNN_model.add(SimpleRNN(50, input_shape=(x_tr.shape[1], 1), activation='relu'))\n","RNN_model.add(Dense(units=1, activation='sigmoid'))  # Adjust units and activation based on your task\n","\n","# Compile the model\n","RNN_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","RNN_model.fit(x_tr, y_tr_encoded, epochs=50, batch_size=32, validation_data=(x_ts, y_ts_encoded))\n","\n","# Evaluate the model\n","accuracy = RNN_model.evaluate(x_ts, y_ts_encoded)[1]\n","\n","print(f'Accuracy: {accuracy}')"]},{"cell_type":"markdown","metadata":{},"source":["**Artificial Neural Network(ANN)**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Assuming x_tr, y_tr, x_ts, y_ts are your training and testing data\n","# Ensure they are numpy arrays or can be converted to numpy arrays\n","# Ensure that y_tr and y_ts are properly encoded (binary labels)\n","\n","# Example label encoding for binary classification\n","label_encoder = LabelEncoder()\n","y_tr_encoded = label_encoder.fit_transform(y_tr)\n","y_ts_encoded = label_encoder.transform(y_ts)\n","\n","# Initialize ANN model\n","ANN_model = Sequential()\n","ANN_model.add(Dense(50, input_shape=(x_tr.shape[1],), activation='relu'))\n","ANN_model.add(Dense(units=1, activation='sigmoid'))  # Adjust units and activation based on your task\n","\n","# Compile the model\n","ANN_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","ANN_model.fit(x_tr, y_tr_encoded, epochs=30, batch_size=32, validation_data=(x_ts, y_ts_encoded))\n","\n","# Evaluate the model\n","accuracy = ANN_model.evaluate(x_ts, y_ts_encoded)[1]\n","\n","print(f'Accuracy: {accuracy}')\n"]},{"cell_type":"markdown","metadata":{},"source":["**Convolutional Neural Network(CNN)**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# Assuming x_tr, y_tr, x_ts, y_ts are your training and testing data DataFrames\n","# Convert DataFrames to numpy arrays\n","x_tr = x_tr.to_numpy()\n","x_ts = x_ts.to_numpy()\n","\n","# Example label encoding for binary classification\n","label_encoder = LabelEncoder()\n","y_tr_encoded = label_encoder.fit_transform(y_tr)\n","y_ts_encoded = label_encoder.transform(y_ts)\n","\n","# Reshape x_tr and x_ts to match the input shape expected by the model\n","x_tr = x_tr.reshape(x_tr.shape[0], -1)  # Flattens the input to a 1D array\n","x_ts = x_ts.reshape(x_ts.shape[0], -1)  # Flattens the input to a 1D array\n","\n","# Initialize the model\n","model = Sequential()\n","model.add(Flatten(input_shape=(x_tr.shape[1],)))  # Flattens the input\n","model.add(Dense(64, activation='relu'))  # Add a dense layer with 64 neurons and ReLU activation\n","model.add(Dense(32, activation='relu'))  # Add another dense layer with 32 neurons and ReLU activation\n","model.add(Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(x_tr, y_tr_encoded, epochs=30, batch_size=32, validation_data=(x_ts, y_ts_encoded))\n","\n","# Evaluate the model\n","accuracy = model.evaluate(x_ts, y_ts_encoded)[1]\n","\n","print(f'Accuracy: {accuracy}')\n"]},{"cell_type":"markdown","metadata":{},"source":["**Long Short-Term Memory(LSTM)**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# Assuming x_tr, y_tr, x_ts, y_ts are your training and testing data\n","# Ensure they are numpy arrays or can be converted to numpy arrays\n","# Ensure that y_tr and y_ts are properly encoded (binary labels)\n","\n","# Example label encoding for binary classification\n","label_encoder = LabelEncoder()\n","y_tr_encoded = label_encoder.fit_transform(y_tr)\n","y_ts_encoded = label_encoder.transform(y_ts)\n","\n","# Reshape x_tr and x_ts if necessary (assuming they are 2D arrays)\n","# Add this if x_tr and x_ts are 1D arrays: x_tr = x_tr.reshape(-1, 1)\n","\n","# Initialize LSTM model\n","LSTM_model = Sequential()\n","LSTM_model.add(LSTM(50, input_shape=(x_tr.shape[1], 1), activation='relu'))\n","LSTM_model.add(Dense(units=1, activation='sigmoid'))  # Adjust units and activation based on your task\n","\n","# Compile the model\n","LSTM_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","LSTM_model.fit(x_tr, y_tr_encoded, epochs=25, batch_size=32, validation_data=(x_ts, y_ts_encoded))\n","\n","# Evaluate the model\n","accuracy = LSTM_model.evaluate(x_ts, y_ts_encoded)[1]\n","\n","print(f'Accuracy: {accuracy}')\n"]},{"cell_type":"markdown","metadata":{},"source":["**RANKING THE TRAINED MODELS ON THE MAE VALUE**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","model_preds = [RT_predictions, knn_predictions, DT_predictions, LR_predictions, SVC_predictions, NB_predictions]\n","model_names = ['Random Forest', 'K-Nearest Neighbors', 'Decision Tree', 'Logistic Regression', 'SVC', 'NB']\n","\n","# Function for comparing different models\n","def score_model(model_preds, y_ts):\n","    return accuracy_score(y_ts, model_preds)\n","\n","# Calculate Accuracy for each model\n","acc_score = []\n","for i in range(len(model_names)):\n","    acc = score_model(model_preds[i], y_ts)\n","    acc_score.append((model_names[i], acc))\n","\n","acc_scores_sorted = sorted(acc_score, key=lambda x: x[1], reverse= True)\n","target_range = y_ts.max() - y_ts.min()\n","\n","# Print ranked model names, MAE scores, and error percentages\n","# for i, (model_name, acc) in enumerate(acc_scores_sorted):\n","#     error_percent = (mae / target_range) * 100  # Calculate error percentage\n","#     print(\"Rank %d: %s - Accuracy: %.4f - : %.2f%%\" % (i+1, model_name, acc, error_percent))\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_preds = [RT_predictions, knn_predictions, DT_predictions, LR_predictions, SVC_predictions, NB_predictions]\n","model_names = ['Random Forest', 'K-Nearest Neighbors', 'Decision Tree', 'Logistic Regression','SVC', 'NB']\n","\n","# Calculate Accuracy for each model\n","acc_score = []\n","for i in range(len(model_names)):\n","    acc = score_model(model_preds[i], y_ts)\n","    acc_score.append((model_names[i], acc))\n","\n","acc_scores_sorted = sorted(acc_score, key=lambda x: x[1], reverse= True)\n","target_range = y_ts.max() - y_ts.min()\n","\n","# Print ranked model names, Accuracy scores, and Accuracy percentages\n","for i, (model_name, acc) in enumerate(acc_scores_sorted):\n","    error_percent = (acc / target_range) * 100  # Calculate error percentage\n","    print(\"Rank %d: %s - ACC: %.4f - Accuracy: %.2f%%\" % (i+1, model_name, acc, error_percent))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":378374,"sourceId":734120,"sourceType":"datasetVersion"}],"dockerImageVersionId":30528,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
