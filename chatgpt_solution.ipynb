{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let's walk through this step by step. We'll parse the data, extract features, train a classifier, and evaluate its performance using a Jupyter Notebook.\n",
    "\n",
    "### Step 1: Parsing the Data\n",
    "\n",
    "The data is structured in HTTP request format. We need to extract useful features from each request, such as the request type (GET/POST), URL, headers, and parameters.\n",
    "\n",
    "### Step 2: Feature Extraction\n",
    "\n",
    "We can extract features such as:\n",
    "- Request type (GET/POST)\n",
    "- Length of URL\n",
    "- Presence of specific parameters\n",
    "- Headers (User-Agent, Accept, etc.)\n",
    "- Length of headers\n",
    "- Content-Length (for POST requests)\n",
    "\n",
    "### Step 3: Preparing the Data\n",
    "\n",
    "We'll combine the normal and anomalous data, label them, and split into training and testing sets.\n",
    "\n",
    "### Step 4: Training a Classifier\n",
    "\n",
    "We'll use a supervised learning classifier. For this example, let's use a Random Forest classifier.\n",
    "\n",
    "### Step 5: Evaluating the Model\n",
    "\n",
    "We'll evaluate the model using accuracy, precision, recall, and F1-score.\n",
    "\n",
    "Here's how this could be implemented in a Jupyter Notebook:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Function to parse HTTP requests and extract features\n",
    "def parse_http_request(request):\n",
    "    lines = request.strip().split('\\n')\n",
    "    method, url, _ = lines[0].split(' ')\n",
    "    headers = {}\n",
    "    content_length = 0\n",
    "    for line in lines[1:]:\n",
    "        if line.startswith('Content-Length'):\n",
    "            content_length = int(line.split(': ')[1])\n",
    "        else:\n",
    "            key, value = line.split(': ', 1)\n",
    "            headers[key] = value\n",
    "    return {\n",
    "        'method': method,\n",
    "        'url_length': len(url),\n",
    "        'num_headers': len(headers),\n",
    "        'content_length': content_length,\n",
    "        'user_agent': headers.get('User-Agent', ''),\n",
    "        'accept': headers.get('Accept', ''),\n",
    "        'accept_language': headers.get('Accept-Language', ''),\n",
    "        'accept_encoding': headers.get('Accept-Encoding', ''),\n",
    "        'host': headers.get('Host', ''),\n",
    "        'connection': headers.get('Connection', '')\n",
    "    }\n",
    "\n",
    "# Load and parse the data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        requests = file.read().split('\\n\\n')\n",
    "    data = [parse_http_request(request) for request in requests if request]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Assuming the data is loaded into two DataFrames: normal_df and anomalous_df\n",
    "normal_df = load_data('normalTrafficTraining.txt')\n",
    "anomalous_df = load_data('anomalousTrafficTest.txt')\n",
    "\n",
    "# Label the data\n",
    "normal_df['label'] = 0\n",
    "anomalous_df['label'] = 1\n",
    "\n",
    "# Combine and split the data\n",
    "df = pd.concat([normal_df, anomalous_df], ignore_index=True)\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert categorical data to numerical data (for simplicity, using label encoding)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "for column in X_train.columns:\n",
    "    if X_train[column].dtype == 'object':\n",
    "        X_train[column] = le.fit_transform(X_train[column])\n",
    "        X_test[column] = le.transform(X_test[column])\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Data Parsing**: We parse the HTTP requests to extract features like method, URL length, number of headers, etc.\n",
    "2. **Data Loading**: Load the normal and anomalous traffic data into separate DataFrames.\n",
    "3. **Labeling and Combining**: Label the normal traffic as 0 and anomalous traffic as 1, then combine them into a single DataFrame.\n",
    "4. **Feature Transformation**: Convert categorical features to numerical using label encoding.\n",
    "5. **Model Training**: Train a Random Forest classifier.\n",
    "6. **Model Evaluation**: Evaluate the classifier's performance using accuracy, precision, recall, and F1-score.\n",
    "\n",
    "This approach should provide a good starting point for your assignment. Feel free to adjust the features and model parameters as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
